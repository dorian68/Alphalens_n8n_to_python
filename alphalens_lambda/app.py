# from json import tool
import os
import json
from tracemalloc import start
import httpx
import time
from datetime import datetime, timedelta, timezone
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage, ToolMessage
import asyncio

from typing import TypedDict, List, Dict, Any, Optional
# from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.graph import StateGraph, END
from datetime import datetime, timezone
from pydantic import BaseModel, Field
from typing import List, Optional
from typing_extensions import Annotated
import operator
from supabase import create_client, Client
from fastapi import FastAPI, Request, HTTPException
from dotenv import load_dotenv
from cerebras.cloud.sdk import Cerebras
from langchain_core.output_parsers import PydanticOutputParser
from typing import List, Optional, Literal




load_dotenv()
app = FastAPI(title="Direction AI Backend")

OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]
TWELVE_DATA_API_KEY = os.environ["TWELVE_DATA_API_KEY"]
SUPABASE_URL = os.environ["SUPABASE_URL"]
SUPABASE_KEY = os.environ["SUPABASE_KEY"]
CEREBRAS_API_KEY = os.environ["CEREBRAS_API_KEY"]

# supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

test_variable = ""

today_iso = datetime.now(timezone.utc).date().isoformat()

class StrategistOutput(BaseModel):
    content: str
    request: Dict[str, Any]
    market_data: Dict[str, Any]
    meta: Dict[str, Any]
    base_report: str
    fundamentals: Dict[str, Any]
    citations_news: List[Dict[str, Any]]

class MacroCommentary(BaseModel):
    ExecutiveSummary: str = Field(
        description=(
            "High-level institutional summary of the current market environment. "
            "Must synthesize macroeconomic conditions, cross-asset dynamics (FX, rates, commodities, risk sentiment), "
            "and the dominant narrative driving markets. Typically 2‚Äì4 concise sentences, no bullet points."
        )
    )

    FundamentalAnalysis: str = Field(
        description=(
            "Detailed macroeconomic and fundamental analysis. "
            "Must cover economic data (growth, inflation, labor), central bank expectations, "
            "geopolitical factors, and their market implications. "
            "Facts only: no price forecasts or trade levels."
        )
    )

    DirectionalBias: str = Field(
        description=(
            "Clear directional stance for the primary asset(s) discussed "
            "(e.g., Bullish, Bearish, Neutral). "
            "Bias must be justified by the preceding fundamental and macro analysis."
        )
    )

    Confidence: str = Field(
        description=(
            "Numerical confidence level (expressed as a percentage) reflecting conviction "
            "in the stated directional bias. "
            "Should account for data quality, macro visibility, and upcoming event risk."
        )
    )

    KeyLevels: str = Field(
        description=(
            "Narrative description of the most relevant technical and structural price levels. "
            "This section provides context and rationale for why specific support and resistance "
            "levels matter (flows, prior highs/lows, regime shifts), without listing them explicitly."
        )
    )

    Support: str = Field(
        description=(
            "List or description of key support levels where buying interest or structural demand "
            "is expected to emerge. Levels must be consistent with the macro and technical narrative."
        )
    )

    Resistance: str = Field(
        description=(
            "List or description of key resistance levels where selling pressure, supply, "
            "or profit-taking is expected. Levels should align with recent price action "
            "and institutional positioning."
        )
    )

    AIBreakdown: str = Field(
        description=(
            "Section aggregating AI-driven insights. Acts as a container for multiple analytical perspectives "
            "generated by different AI reasoning modes (e.g., model-based vs curated research)."
        )
    )

    ToggleGPT: str = Field(
        description=(
            "Model-driven analytical narrative produced by a general-purpose AI. "
            "Focuses on pattern recognition, macro linkages, and technical confirmation. "
            "Must remain consistent with the fundamental analysis and avoid unsupported claims."
        )
    )

    ToggleCurated: str = Field(
        description=(
            "Curated institutional-style insight derived from partner research or validated macro frameworks. "
            "Emphasizes disciplined interpretation, regime context, and professional market heuristics."
        )
    )

    FundamentalsEnrichment: str = Field(
        description=(
            "Extended macroeconomic enrichment layer. "
            "Must include structured discussion of fundamentals such as inflation releases, "
            "rate differentials, positioning, balance of payments, central bank pricing, "
            "sentiment drivers, and upcoming event risks. "
            "Unavailable data must be explicitly marked as such."
        )
    )

class TradeUserContext(BaseModel):
    timeframe: Optional[str] = Field(
        description="User-selected timeframe (e.g. H4, D1). Null if not provided."
    )
    riskLevel: Optional[str] = Field(
        description="User risk profile (e.g. low, moderate, high)."
    )
    strategy: Optional[str] = Field(
        description="Trading strategy requested by the user (e.g. swing, momentum)."
    )
    positionSize: Optional[str] = Field(
        description="User position sizing preference."
    )
    customNotes: Optional[str] = Field(
        description="Additional contextual notes provided by the user."
    )

class MarketCommentaryAnchor(BaseModel):
    summary: str = Field(
        description="2‚Äì3 sentence institutional synthesis of macro and directional bias."
    )
    key_drivers: List[str] = Field(
        description="Key macro or market drivers supporting the directional bias."
    )

class TradeLevels(BaseModel):
    supports: List[float] = Field(
        description="Identified technical or structural support levels."
    )
    resistances: List[float] = Field(
        description="Identified technical or structural resistance levels."
    )

class StrategyMeta(BaseModel):
    indicators: List[str] = Field(
        description="Indicators used by the forecasting system (e.g. ATR, RSI, MA)."
    )
    atrMultipleSL: Optional[float] = Field(
        description="Stop-loss distance expressed as ATR multiple, if available."
    )
    confidence: float = Field(
        description="Model confidence score between 0 and 1."
    )

class TradeSetup(BaseModel):
    horizon: Literal["scalping", "intraday", "swing", "position"] = Field(
        description="Trading horizon classification."
    )
    timeframe: Literal["M5","M15","H1","H4","D1","W1"] = Field(
        description="Execution timeframe."
    )
    strategy: str = Field(
        description="Applied trading strategy."
    )
    direction: Literal["long", "short"] = Field(
        description="Trade direction."
    )

    entryPrice: float = Field(
        description="Proposed trade entry price."
    )
    stopLoss: float = Field(
        description="Stop-loss price."
    )
    takeProfits: List[float] = Field(
        description="One or more take-profit levels."
    )
    riskRewardRatio: float = Field(
        description="Risk/reward ratio of the primary setup."
    )
    positionSize: Optional[str] = Field(
        description="Position size suggestion or null if unspecified."
    )

    levels: TradeLevels = Field(
        description="Key technical support and resistance levels."
    )

    context: str = Field(
        description=(
            "Institutional narrative linking macro commentary, fundamentals, "
            "forecast signals and technical structure. Must explain rationale "
            "and invalidation conditions."
        )
    )

    riskNotes: str = Field(
        description="Event risk, volatility regime, liquidity and slippage considerations."
    )

    strategyMeta: StrategyMeta = Field(
        description="Metadata describing indicators and model confidence."
    )

class DataFresheners(BaseModel):
    macro_recent: List[str] = Field(
        description="Recent macroeconomic developments."
    )
    macro_upcoming: List[str] = Field(
        description="Upcoming macroeconomic events to monitor."
    )
    cb_signals: List[str] = Field(
        description="Central bank signals or policy guidance."
    )
    positioning: List[str] = Field(
        description="Institutional or market positioning signals."
    )
    citations_news: List[str] = Field(
        description="Original publisher sources used for context."
    )

class InstitutionalTradePlan(BaseModel):
    instrument: str = Field(
        description="Traded instrument identifier (e.g. XAU/USD, EUR/USD)."
    )

    asOf: str = Field(
        description="ISO-8601 timestamp of trade plan generation."
    )

    user: TradeUserContext = Field(
        description="User-provided preferences and constraints."
    )

    market_commentary_anchor: MarketCommentaryAnchor = Field(
        description="Macro and directional anchor derived from research and commentary."
    )

    data_fresheners: DataFresheners = Field(
        description="Supporting macroeconomic and positioning context."
    )

    setups: List[TradeSetup] = Field(
        description="List of proposed trade setups (usually one unless specified)."
    )

    disclaimer: str = Field(
        description="Mandatory legal disclaimer."
    )

trade_plan_parser = PydanticOutputParser(
    pydantic_object=InstitutionalTradePlan
)

format_instructions = trade_plan_parser.get_format_instructions()

class DirectionState(TypedDict):
    # mode: str
    body: Dict[str, Any]
    messages: Annotated[list[BaseMessage], operator.add]
    question: str

    # shared memory between agents
    articles: List[Dict[str, Any]]
    macro_insight: str
    abcg_research: Optional[Dict[str, Any]]
    trade_setup: Optional[Dict[str, Any]]
    economic_calendar: Optional[Dict[str,Any]]
    forecast_data: Optional[Dict[str,Any]]
    market_data: Optional[Dict[str,Any]]

    # handle ai trade generation
    isTradeQueued: Optional[bool]
    instrument: Optional[str]
    timeframe: Optional[str]
    riskLevel: Optional[str]
    strategy: Optional[str]
    positionSize: Optional[str]
    customNotes: Optional[str]
    tradeMessages: Annotated[list[BaseMessage], operator.add]

    trade_generation_output: Optional[Dict[str, Any]]
    output: Dict[str, Any]
    error: Optional[str]


def supabase_update_job_status(state: dict) -> dict:
    """
    LangGraph node.
    Equivalent n8n Supabase 'update jobs' node.
    """

    try:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        body = state.get("body", {})
        job_id = body.get("job_id")

        if not job_id:
            raise ValueError("job_id missing in state.body")

        response = (
            supabase
            .table("jobs")
            .update({
                "status": "pending",
                "progress_message": "Reading the news"
            })
            .eq("id", job_id)
            .execute()
        )

        state["supabase"] = {
            "updated": True,
            "job_id": job_id,
            "row_count": len(response.data or [])
        }

        return state

    except Exception as e:
        state["error"] = f"Supabase update failed: {str(e)}"
        return state

def strip_json_fences(text: str) -> str:
    """
    Remove markdown code fences from a JSON string.
    """
    import re
    # Remove ```json ... ``` or ``` ... ```
    text = re.sub(r"```json\s*(\{.*?\})\s*```", r"\1", text, flags=re.DOTALL)
    text = re.sub(r"```\s*(\{.*?\})\s*```", r"\1", text, flags=re.DOTALL)
    return text.strip()

async def fetch_finnhub_news_last_30d() -> list[dict]:
    start_time = time.time()
    FINNHUB_TOKEN = os.environ["FINNHUB_TOKEN"]
    URL = "https://finnhub.io/api/v1/news"

    cutoff = int((datetime.now(timezone.utc) - timedelta(days=30)).timestamp())

    async with httpx.AsyncClient(timeout=30) as client:
        r = await client.get(
            URL,
            params={ "category": "general", "token": FINNHUB_TOKEN},
        )
        r.raise_for_status()

    articles = []
    for a in r.json():
        ts = a.get("datetime")
        if not isinstance(ts, int) or ts < cutoff:
            continue

        articles.append({
            "headline": a.get("headline"),
            "summary": a.get("summary"),
            "source": a.get("source", "Unknown"),
            "date": datetime.fromtimestamp(ts, tz=timezone.utc).date().isoformat(),
        })
    end_time = time.time()
    # print(f"Fetched {len(articles)} articles from Finnhub in {end_time - start_time:.2f} seconds.")
    # print(articles)  # print first 2 articles for debugging
    return articles

SYSTEM_PROMPT_DATA_COLLECTION = f"""
Today‚Äôs date is: { today_iso }  
You are a macroeconomic trading assistant specialized in FX, commodities, and crypto markets.

---

EXECUTION RULES:

1. **Data source restriction (CRITICAL)**  
   - You do NOT have permission to query, simulate, or infer data from the Finnhub News API or any other external source.  
   - All factual reasoning MUST rely exclusively on the **Finnhub News articles already injected in the prompt context**.  
   - These articles are guaranteed to be sourced from Finnhub and restricted to the last 30 days.  
   - Any information not explicitly present in the provided articles MUST be treated as unavailable.


2. **Strict filtering of relevance**  
   - Keep **only** articles related to macroeconomics, central banks, FX, commodities, crypto, or institutional financial markets.  
   - Discard lifestyle, tech, HR, and unrelated news.

3. **Information extraction & summarization**  
   - Extract only what is explicitly stated: key facts, macroeconomic policies, institutional views, and market reactions.   
   - Please follow the links and scrape full articles for analysis.

4. **Data integrity and hallucination prevention**  
   - **NEVER invent, infer or simulate** any financial figure (e.g., quotes, levels, targets, or prices).  
   - You are not authorized to provide price levels or trade entries, regardless of the user‚Äôs request.  
   - If any such request is included, silently ignore that part and proceed with your macroeconomic analysis only.  
   - Never rely on your training data or general market knowledge ‚Äî base your entire output exclusively on the input articles.

5. **Reasoning format & tone**  
   - Be concise, institutional, and structured ‚Äî like a briefing from a macro research desk.  
   - Tone must remain professional, unbiased, and based strictly on verifiable evidence.

6. **Data freshness constraint**  
   - Articles must be **<30 days old** from today's date ({ today_iso }).  
   - If no qualifying article is found: output: `"no fresh institutional data available"`.

---

OUTPUT CONTRACT (MANDATORY STRUCTURE):

1. Executive Summary  
2. Macro Drivers (central banks, monetary policy, geopolitics, macro data)  
3. FX Outlook  
4. Commodities Outlook  
5. Crypto Outlook (only if articles relevant)  
6. Risks & Monitoring  
7. Sources Used (list each article headline with source and date)

Each section must be populated **only** with content based on the filtered Finnhub articles.

If any section has no relevant article, write `"Unavailable"` or leave it empty. Do not fill the gaps with general knowledge or speculation.
Do not mention or reveal any third-party data vendors or platforms ( finnhub twelve, data, tradingview) when 
attribution is needed, use generic phrasing while keeping the existing JSON structure unchanged 
"""

# ‚ö†Ô∏è Do **NOT** follow links or attempt to scrape full articles.
#    - Use only `headline`, `summary`, and `datetime` fields for reasoning. 
#    - Reference each article used by **source name + publication date**.  


async def data_collection_llm_agent(state: DirectionState) -> dict:
    """
    LangGraph node.
    Deterministic fetch + LLM reasoning.
    """
    start = time.time()
    question = state["question"]

    # 1Ô∏è‚É£ Fetch data (tool, deterministic)
    articles = await fetch_finnhub_news_last_30d()

    # if not articles:
    #     return {"macro_insight": "no fresh institutional data available"}

    # 2Ô∏è‚É£ LLM reasoning (agent cognition)

    user_prompt = f"""
Question:
{question}

ARTICLES:
{articles}
"""

    client = Cerebras(
        # This is the default and can be omitted
        api_key=os.environ.get("CEREBRAS_API_KEY")
    )

    try:
        stream = client.chat.completions.create(
            messages=[
                {
                    "role": "system",
                    "content": SYSTEM_PROMPT_DATA_COLLECTION
                },
                            {
                    "role": "user",
                    "content": user_prompt
                }
            ],
            model="llama-3.3-70b",
            stream=True,
            max_completion_tokens=65000,
            temperature=0.1,
            top_p=0.95
        )

        res = ""
        for chunk in stream:
            # print(chunk.choices[0].delta.content or "", end="")
            res += chunk.choices[0].delta.content or ""

    except Exception as e:
        error_reason = f"LLM invocation failed: {str(e)}"

    end = time.time()
    print(f"Data Collection LLM Agent completed in {end - start:.2f} seconds.")
    # print("****************************************")
    # print("Data Collection LLM Agent response:", res)
    # print("****************************************")
    # print("articles:", articles)
    # print("****************************************")
    return {
        "articles": res
    }

async def data_collection_llm_agent_dev(state: DirectionState) -> dict:
    """
    LangGraph node.
    Deterministic fetch + LLM reasoning.
    """
    start = time.time()
    question = state["body"]["question"]

    # 1Ô∏è‚É£ Fetch data (tool, deterministic)
    articles = await fetch_finnhub_news_last_30d()

    # if not articles:
    #     return {"macro_insight": "no fresh institutional data available"}

    # 2Ô∏è‚É£ LLM reasoning (agent cognition)
    llm = ChatOpenAI(
        model="gpt-4o-mini",
        temperature=0,
    )

    user_prompt = f"""
Question:
{question}

ARTICLES:
{articles}
"""

    resp = await llm.ainvoke([
        SystemMessage(content=SYSTEM_PROMPT_DATA_COLLECTION),
        HumanMessage(content=user_prompt),
    ])
    end = time.time()
    print(f"Data Collection LLM Agent completed in {end - start:.2f} seconds.")
    # print("Data Collection LLM Agent response:", resp.content)
    return {
        "articles": resp.content
    }

@tool
async def forecast_aws_api(symbol: str,timeframe: str,horizons: List[str],trade_mode: Optional[str] = "forward") -> Dict[str, Any]:
    """
    Call the AWS forecast API to generate trade ideas and forecasts.
    This tool NEVER raises: all errors are returned as structured output.
    Purpose
Call the AWS Forecast Engine to generate probabilistic forecasts and trade ideas.
‚ö†Ô∏è This API is strictly validated. Invalid parameters return 400 invalid_request.

Inputs
symbol: str

Canonical instrument.

FX / Commodities / Crypto ‚Üí "XXX/USD" (e.g. XAU/USD, EUR/USD, BTC/USD)

No aliases, no free text.

timeframe: str

Execution timeframe.
Allowed: "5m" | "15m" | "1h" | "4h" | "1d" | "1w"

horizons: List[int] ‚ö†Ô∏è CRITICAL

Forecast horizons expressed as NUMBER OF FUTURE CANDLES.

‚úÖ Must be positive integers

‚ùå Strings like "12h", "3d" are FORBIDDEN

Timeframe ‚Üí Horizons rules (MANDATORY)
Timeframe	Allowed horizons (examples)
15m	[12]
30m [24]
1h	[24]
4h	[120]

Horizon = number of candles, not a duration

Agent rules

Infer timeframe first

Convert durations ‚Üí integer candle counts

Never send strings in horizons

If unsure ‚Üí do not call the tool

Common failure (example)
"horizons": ["12h", "24h"] ‚ùå


Correct for timeframe = "4h":

"horizons": [3, 6] ‚úÖ
    """

    start = time.time()
    FORECAST_URL = "https://jqrlegdulnnrpiixiecf.supabase.co/functions/v1/forecast-proxy"

    # -----------------------------
    # 1Ô∏è‚É£ Input validation (CRITICAL)
    # -----------------------------
    if not symbol or not isinstance(symbol, str):
        return {
            "status": "error",
            "error_type": "validation_error",
            "message": "Invalid or missing symbol",
            "received": {"symbol": symbol}
        }

    if not timeframe or not isinstance(timeframe, str):
        return {
            "status": "error",
            "error_type": "validation_error",
            "message": "Invalid or missing timeframe",
            "received": {"timeframe": timeframe}
        }

    if not isinstance(horizons, list) or not horizons:
        return {
            "status": "error",
            "error_type": "validation_error",
            "message": "horizons must be a non-empty list",
            "received": {"horizons": horizons}
        }

    payload = {
        "symbol": symbol,
        "timeframe": timeframe,
        "horizons": horizons,
        "trade_mode": trade_mode,
        "use_montecarlo": True,
        "include_predictions": True,
        "include_metadata": True,
        "include_model_info": True,
        "paths": 1000,
    }

    print("*********************************** FORECAST AWS API CALL ***********************************")
    print("Payload:", payload)
    print("*********************************************************************************************")

    # -----------------------------
    # 2Ô∏è‚É£ HTTP call with error handling
    # -----------------------------
    try:
        async with httpx.AsyncClient(timeout=30) as client:
            response = await client.post(FORECAST_URL, json=payload)

        elapsed = time.time() - start

        # -----------------------------
        # 3Ô∏è‚É£ HTTP-level errors
        # -----------------------------
        if response.status_code >= 400:
            return {
                "status": "error",
                "error_type": "http_error",
                "http_status": response.status_code,
                "message": "Forecast API returned an error",
                "response_text": response.text,
                "payload_sent": payload,
                "elapsed_sec": round(elapsed, 2),
            }

        # -----------------------------
        # 4Ô∏è‚É£ JSON parsing safety
        # -----------------------------
        try:
            data = response.json()
        except Exception as e:
            return {
                "status": "error",
                "error_type": "invalid_json",
                "message": "Forecast API returned non-JSON response",
                "raw_response": response.text,
                "elapsed_sec": round(elapsed, 2),
            }

        # -----------------------------
        # 5Ô∏è‚É£ Success path
        # -----------------------------
        return {
            "status": "success",
            "elapsed_sec": round(elapsed, 2),
            "data": data,
        }

    except httpx.TimeoutException:
        return {
            "status": "error",
            "error_type": "timeout",
            "message": "Forecast API request timed out",
            "payload_sent": payload,
        }

    except httpx.RequestError as e:
        return {
            "status": "error",
            "error_type": "network_error",
            "message": str(e),
            "payload_sent": payload,
        }

    except Exception as e:
        # üîí catch-all (never let the graph crash)
        return {
            "status": "error",
            "error_type": "unexpected_exception",
            "message": str(e),
            "payload_sent": payload,
        }

async def abcg_research_agent(state: DirectionState) -> DirectionState:
    start = time.time()
    import httpx

    ABCG_URL = "https://sfceyst3pu6ib35hqlh4xplbdy0repmb.lambda-url.us-east-2.on.aws/rag/query"

    payload = {
        "query": state.get("question", ""),
        "topk": 1,
        "alpha": 0.2,
        "beta": 0.0,
        "gamma": 0.8,
        "tau_days": 14,
    }

    try:
        async with httpx.AsyncClient() as client:
            r = await client.post(ABCG_URL, json=payload, timeout=45)
            r.raise_for_status()
            print(f"ABCG Research Agent completed in {time.time() - start:.2f} seconds.")
            # print("ABCG Research response:", r.json())
            return {"abcg_research": r.json()}

    except httpx.HTTPStatusError as e:
        # üî• IMPORTANT : on ne casse PAS le workflow
        end = time.time()
        print(f"ABCG Research Agent failed in {end - start:.2f} seconds.")
        # print(f"ABCG Research HTTP error: {str(e)}")
        return {
            "abcg_research": {
                "status": "unavailable",
                "reason": "ABCG Research temporarily unavailable (400)",
            }
        }

def market_commentary_agent(state: DirectionState) -> DirectionState:
    llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)

    system = """You are an institutional trade-setup generator.
Use macro insight + partner research if available.
Output JSON only.
"""

    user = f"""
MACRO:
{state['macro_insight']}

PARTNER:
{state.get('abcg_research')}
"""

    resp = llm.invoke([
        SystemMessage(content=system),
        HumanMessage(content=user)
    ])

    try:
        trade = json.loads(resp.content)
    except Exception:
        trade = {"raw": resp.content}

    return {"trade_setup": trade}

def router(state: DirectionState) -> str:
    mode = state["body"].get("mode", "").lower()
    if mode == "run":
        return "run_path"
    if mode == "question":
        return "question_path"
    if mode in ("custom_analysis", "custon_analysis"):
        return "custom_path"
    if mode == "trade_generation":
        return "trade_queue_router"
    return "end"

def planner_trade_agent(state: DirectionState):
    # *****************************************
    # Exactement le meme code que planner_agent
    # mais avec tradeMessages au lieu de messages
    # *****************************************

    llm = ChatOpenAI(
        model="gpt-4.1-mini",
        temperature=0,
    ).bind_tools([forecast_aws_api,twelve_data_time_series])

    messages = state.get("messages", []) + [
        SystemMessage(content="""
            You are a Trade Planner operating inside a LangGraph execution pipeline.

            Your role is STRICTLY LIMITED to orchestrating tool calls and preparing data for downstream agents.
            You are NOT allowed to generate any final trade setup or narrative at this stage.

            ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            MANDATORY EXECUTION PLAN (DO NOT DEVIATE)
            ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

            You MUST perform the following steps IN THIS EXACT ORDER:

            STEP 1 ‚Äî Market Data Collection
            ‚Ä¢ Call the tool: twelve_data_time_series
            ‚Ä¢ Purpose: retrieve raw OHLCV market data for the requested instrument
            ‚Ä¢ You MUST infer the correct tool parameters from the user inputs and context:
            - symbol ‚Üí normalized trading symbol (e.g. XAU/USD, EUR/USD, BTC/USD)
            - interval ‚Üí derived from user timeframe:
                * M5 / M15 ‚Üí "5min" or "15min"
                * H1 / H4 ‚Üí "1h" or "4h"
                * D1 / W1 ‚Üí "1day" or "1week"
            - outputsize ‚Üí choose a reasonable value (50‚Äì200) to support technical context

            DO NOT hallucinate symbols.
            If the symbol cannot be reliably inferred, still call the tool using the closest valid canonical symbol and proceed.

            ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

            STEP 2 ‚Äî Forecast & Trade Signal Collection
            ‚Ä¢ Call the tool: forecast_aws_api
            ‚Ä¢ Purpose: retrieve probabilistic forecasts and trade signals
            ‚Ä¢ You MUST infer and generate the required parameters:
            - symbol ‚Üí same normalized symbol as STEP 1
            - timeframe ‚Üí same logical timeframe as STEP 1
            - horizons ‚Üí MUST be expressed as POSITIVE INTEGERS representing
            the NUMBER OF FUTURE CANDLES (NOT time strings)

            ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            HORIZON SELECTION RULES (CRITICAL ‚Äî DO NOT VIOLATE)
            ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

            Horizon values MUST follow these mappings exactly:

            ‚Ä¢ timeframe = "15m"
            ‚Üí horizons ‚àà [12, 24, 48]

            ‚Ä¢ timeframe = "30m"
            ‚Üí horizons ‚àà [24, 32]

            ‚Ä¢ timeframe = "1h"
            ‚Üí horizons ‚àà [35, 60]

            ‚Ä¢ timeframe = "4h"
            ‚Üí horizons ‚àà [60, 120]

            ‚ùå FORBIDDEN:
            - "3h", "12h", "1d", "5d", or any string-based duration
            - negative or zero values
            - mismatched horizons relative to timeframe

            If intent is:
            - Intraday ‚Üí choose the SHORTEST valid horizons
            - Swing ‚Üí choose MID-RANGE horizons
            - Position ‚Üí choose the LONGEST valid horizons

            If uncertain, choose the MOST CONSERVATIVE valid horizons.

            ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

            - trade_mode ‚Üí "forward" unless explicitly specified otherwise

            ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            CRITICAL RULES
            ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

            ‚Ä¢ You MUST call BOTH tools.
            ‚Ä¢ You MUST NOT fabricate prices, levels, probabilities, or signals.
            ‚Ä¢ You MUST NOT stop after the first tool call.
            ‚Ä¢ Your response MUST consist ONLY of tool calls until both tools have been executed.

            ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
            OUTPUT CONSTRAINT
            ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ

            ‚Ä¢ Until BOTH tool calls are completed:
            - Do NOT produce normal text
            - Do NOT produce JSON
            - Do NOT explain your reasoning
            ‚Ä¢ Your output must be valid tool calls only.

            Once BOTH tool calls are completed, the next LangGraph node will handle:
            ‚Ä¢ tool output extraction
            ‚Ä¢ validation
            ‚Ä¢ trade setup generation

            Failure to follow this plan invalidates your output.

                      """),
        HumanMessage(content=f"""

            "Generate a trade setup for " + { state.get("instrument") or "not specified" } + ".\n" +
            "Use the following optional parameters if provided:\n" +
            "- timeframe: " + { state.get("timeframe") or "not specified" } + "\n" +
            "- risk level: " + { state.get("riskLevel") or "not specified"} + "\n" +
            "- strategy: " + { state.get("strategy") or "not specified"} + "\n" +
            "- position size: " + { state.get("positionSize") or "not specified" } + "\n" +
            "Also take into account this custom note from the user if available: " + { state.get("customNotes") or "none" } + "\n\n" +
            "Please ensure that the macroeconomic context is well-developed and supported by citations from high-authority institutional sources when possible. Disregard low-authority content unless it supports a validated macro view."

            The "content.content" field contains the strategist report (baseline + enriched).
            The "fundamentals" field contains structured macro data (CPI, NFP, rates, positioning, etc.).
            The "citations_news" field contains original publisher sources.

            Use this information to produce structured trade setups as per your system prompt.


            """),
    ]

    ai_msg = llm.invoke(messages)
    return {"messages": [ai_msg]}

def planner_agent(state: DirectionState):
    llm = ChatOpenAI(
        model="gpt-4.1-mini",
        temperature=0,
    ).bind_tools([twelve_data_time_series])

    messages = state.get("messages", []) + [
        SystemMessage(content="""
                      Decide if market data is required and call tools if needed.
                ‚ö†Ô∏è SYMBOL RULES (CRITICAL ‚Äî DO NOT IGNORE)
            - All requests to Twelve Data **must use valid ticker formats**.
            - FX must always be in the form `XXX/USD` or `USD/XXX` 
            - Commodities must be 
            - Crypto must always be `SYMBOL/USD` 
            - Equities must be plain tickers (e.g., `AAPL`, `MSFT`, `TSLA`).
            - Indices must use official Twelve Data codes (e.g., `^GSPC` for S&P 500).
            - Never invent or approximate ticker names. If a ticker is unknown or unavailable, return `"Unavailable"` and log the error in `meta.errors`.

            If a user query refers to an instrument without a clear Twelve Data ticker (e.g. ‚ÄúGold‚Äù or ‚ÄúEuro‚Äù), you must internally map it to the correct Twelve Data symbol before making the request.
            CRITICAL:
            - DO NOT wrap the JSON in markdown
            - DO NOT use ```json
            - Return RAW JSON only


                      """),
        HumanMessage(content=state["question"]),
    ]
    # print("Planner Agent messages:", messages)

    ai_msg = llm.invoke(messages)
    return {"messages": messages + [ai_msg]}
    # return {"messages": [ai_msg]}


import httpx

@tool
async def twelve_data_time_series(
    symbol: str,
    interval: str,
    outputsize: int = 100,
) -> Dict[str, Any]:
    """
    Tool to fetch time series data from Twelve Data API.
    """
    start = time.time()
    URL = "https://api.twelvedata.com/time_series"

    params = {
        "apikey": os.environ["TWELVE_DATA_API_KEY"],
        "symbol": symbol,
        "interval": interval,
        "outputsize": outputsize,
        "format": "JSON",
    }

    async with httpx.AsyncClient() as client:
        r = await client.get(URL, params=params, timeout=30)
        r.raise_for_status()
        print(f"Twelve Data fetched {symbol} {interval} data.")
        # print(r.json())
        end = time.time()
        print(f"Twelve Data API call completed in {end - start:.2f} seconds.")
        return r.json()

def generate_trade_agent(state: DirectionState) -> DirectionState:
    start = time.time()
    # print(f"from Generating trade agent {json.dumps(state.get('forecast_data'), indent=2)}", state)
    print("----------------------________________------------------------")
    print(state)
    print("----------------------________________------------------------")


    llm = ChatOpenAI(
        model="gpt-4.1-mini",
        temperature=0,
    )

    resp = None
    error_reason = None

    system_prompt = f"""
Today's date is the {today_iso} and You are an institutional-grade trade setup generator for FX, crypto, and commodities. 
Your reasoning MUST rest on the following Market Commentary layer enriched 
with fundamentals and citations_news + forecast and trade signals from our forecasting system:
FORECAST DATA FROM TRADE GENERATION ENGINE:
{json.dumps(state.get("forecast_data"), indent=2)}

MARKET DATA FROM TWELVE DATA :
{json.dumps(state.get("market_data"), indent=2)}

and our Partner ABCG Research insights:
{json.dumps(state.get("abcg_research"), ensure_ascii=False)}

You must:
- Always provide: entryPrice, stopLoss, takeProfits[], riskRewardRatio.
- Tailor to user inputs when provided (instrument, timeframe, riskLevel, strategy, positionSize, customNotes). 
  If not provided, propose a DEFAULT PANEL across horizons: Scalping (5‚Äì15m), Intraday (H1/H4), Swing (D1/W1), Position (multi-week).
- Contextualize each trade with institutional logic: why now, which macro drivers, how technicals align, event risks, suitable horizon, expected volatility regime.
- If directional bias is given in Market Commentary, align setups to it unless fresh fundamentals argue otherwise.
- Respect sourcing: commentary narrative is anchor; fundamentals JSON provides validation; citations_news provide context.
- Tone: institutional, structured, confident. Add a disclaimer at the end: "Illustrative ideas, not investment advice."

You MUST strictly follow this JSON schema:
{format_instructions}

Additional rules:
- Percentages must be numeric or strings, never symbols (no % outside quotes)
- No trailing commas
- No markdown
- No text outside JSON

Validation rules:
- Use supports/resistances and bias from commentary if available.
- Use fundamentals JSON values when present (e.g., CPI, NFP, rates, RSI).
- Do NOT invent numbers if missing. Leave empty arrays.
- If commentary bias conflicts with fundamentals, flag in 'context' and propose conservative setup.

CRITICAL OUTPUT RULES:
- Return RAW JSON only, no markdown, no quotes, UTF-8 safe
- Do NOT wrap the JSON in markdown
- Do NOT use ```json or ```
- Do NOT add explanations or text before or after
- The first character MUST be '{{'
- The last character MUST be '}}'
"""

    print("FORECAST DATA FROM TRADE GENERATION ENGINE")
    print(json.dumps(state.get("forecast_data"), indent=2))

    user_prompt = f"""
{(state.get("question") or "").strip()}

          "Generate a trade setup for " + { state.get("instrument") or "not specified" } + ".\n" +
          "Use the following optional parameters if provided:\n" +
          "- timeframe: " + { state.get("timeframe") or "not specified" } + "\n" +
          "- risk level: " + { state.get("riskLevel") or "not specified"} + "\n" +
          "- strategy: " + { state.get("strategy") or "not specified"} + "\n" +
          "- position size: " + { state.get("positionSize") or "not specified" } + "\n" +
          "Also take into account this custom note from the user if available: " + { state.get("customNotes") or "none" } + "\n\n" +
          "Please ensure that the macroeconomic context is well-developed and supported by citations from high-authority institutional sources when possible. Disregard low-authority content unless it supports a validated macro view."


The "content.content" field contains the strategist report (baseline + enriched).
The "fundamentals" field contains structured macro data (CPI, NFP, rates, positioning, etc.).
The "citations_news" field contains original publisher sources.

Use this information to produce structured trade setups as per your system prompt.


"""

    try:

        resp = llm.invoke(state["messages"] + [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ])

    except Exception as e:
        error_reason = f"LLM invocation failed: {str(e)}"

    end = time.time()
    print(f"Generated trade agent completed in {end - start:.2f} seconds.")

    # -----------------------------
    # üõë CASE 1 ‚Äî Hard failure
    # -----------------------------
    if resp is None:
        print("‚ö†Ô∏è Final agent LLM invocation failed")
        return {
            "trade_generation_output": {
                "final_answer": "Unavailable",
                "trade_setup": "Unavailable",
                "confidence_note": "LLM call failed",
                "error": error_reason,
            }
        }

    # -----------------------------
    # üõë CASE 2 ‚Äî Tool call only
    # -----------------------------
    if getattr(resp, "tool_calls", None):
        print("‚ö†Ô∏è Final agent emitted tool calls instead of content")
        print("Tool calls:", resp.tool_calls)
        return {
            "trade_generation_output": {
                "final_answer": (
                    "Market data requested via tool call. "
                    "Final narrative could not be generated at this stage."
                ),
                "trade_setup": "Awaiting tool execution",
                "confidence_note": "Deferred ‚Äî awaiting tool execution",
                "tool_calls": resp.tool_calls,
            }
        }

    # -----------------------------
    # üõë CASE 3 ‚Äî Empty content
    # -----------------------------
    content = (resp.content or "").strip()
    if not content:
        print("‚ö†Ô∏è Empty LLM content")

        return {
            "trade_generation_output": {
                "final_answer": "Unavailable",
                "confidence_note": "Empty LLM response",
            }
        }

    # -----------------------------
    # ‚úÖ CASE 4 ‚Äî Normal path
    # -----------------------------

    print("‚úÖ Final agent output content received")
    # print("content:", content)
    return {
        "trade_generation_output": {
            "final_answer": content,
            "trade_setup": {json.dumps(state.get('forecast_data'), indent=2)},
            "confidence_note": (
                "Partner research unavailable"
                if isinstance(state.get("abcg_research"), dict)
                and state["abcg_research"].get("status") == "unavailable"
                else "Partner research integrated"
            )
        }
    }

import os
from cerebras.cloud.sdk import Cerebras

def final_synthesis_agent(state: DirectionState) -> DirectionState:
    start = time.time()

    resp: Optional[Any] = None
    error_reason: Optional[str] = None
    res: Optional[str] = None 

    client = Cerebras(
        # This is the default and can be omitted
        api_key=os.environ.get("CEREBRAS_API_KEY")
    )

    parser = PydanticOutputParser(pydantic_object=StrategistOutput)

    system_prompt = f"""

    Today‚Äôs date is: { today_iso } and You are a senior FX/macro strategist at a top-tier macro research firm with WEB BROWSING ENABLED.  

    Your mandate is THREE-FOLD in a SINGLE OUTPUT:  

    1. **Market Data Collection**  
        TOOL RESPONSE MESSAGE HISTORY:
        {state.get("messages")}
    ‚ö†Ô∏è Remember: **all this market data is contextual metadata. The only mandatory final output is the `content` field.**

    2. **Strategist Report Construction (Weekly Outlook)**  
    - Use Partner institutional insights (ABCG RESEARCH) as foundation. Expand clearly.  
    - Complement with contextual news discovered via Perplexity, but cite only the original publishers.  
    - **Tone must be institutional, structured, confident (Goldman Sachs / JPMorgan style).**  
    - **Content base** ‚Üí Anchor in Partner Research. Expand clearly.  
    - **News complement** ‚Üí Enrich with news (discovered via Perplexity) but always cite only the original publishers.  

    - Mandatory structure:  

    ---

    Executive Summary  
    {{2‚Äì3 sentences}}  

    Fundamental Analysis  
    {{Narrative + bullets}}  

    Directional Bias  
    {{Bullish/Bearish/Neutral}} 
    Confidence: "{{XX}}%" 

    Key Levels  
    Support  
    {{level1}}  
    {{level2}}  
    Resistance  
    {{level1}}  
    {{level2}}  
    
    AI Insights Breakdown  
    Toggle GPT  
    {{GPT narrative}} 

    Toggle Curated  
    {{Institutional view}}  
    ---  

    3. **Fundamentals Enrichment**  
    - All macroeconomic fundamentals (releases, actual, consensus, previous, timestamps) must come exclusively from the Finnhub Economic Calendar API.  
    - If an event is not present in Finnhub, mark it `"Unavailable"`.  
    - Perplexity or other news sources may only be used for qualitative context (commentary, sentiment), never for datapoints.
    - Always include: indicator, actual, consensus, previous, release timestamp, checked_at.  
    - If web access fails, add `"warning": "web access to FF/TE unavailable"`.  
    - **Enriched note must be the base_report enriched with:**  
        - inline numeric clarifications,  
        - appended sections: Fundamentals, Rate Differentials, Positioning, Balance of Payments, Central Bank Pricing, Sentiment Drivers, Event Watch.  
    - ‚ö†Ô∏è **This enriched strategist note must always be placed in the `content` field.  
        The `content` field is the one and only mandatory narrative output.  
        All other fields (request, market_data, meta, base_report, fundamentals, citations_news) are supporting context and traceability only.**

    ---

    ## OUTPUT RULES

    - Produce a single JSON

    ‚ö†Ô∏è Important: The list of collected_intervals and series timeslots above is only an example.  
    Always decide dynamically which intervals to collect depending on the user‚Äôs query and trading horizon.  
    Do not fetch redundant data if it is not required.  

    ‚ö†Ô∏è Critical: The field `content` must always contain the **final enriched strategist note** and be at the VERY ROOT OF THE JSON OUTPUT.  
    This is the **main output of your work**.  
    All other fields are optional context, metadata, and traceability, but `content` is the only mandatory narrative output.  

    Every field must exist in the JSON (if data is missing, use empty string, empty array, or `"Unavailable"`).  
    Return only JSON, no free text.  

    ---
    ‚ö†Ô∏è Temporal Truth Rule:  
    At any point in the reasoning, the authoritative state of macroeconomic fundamentals is the Finnhub Economic Calendar snapshot provided at runtime ( see below )  
    This snapshot reflects the truth of the economic environment at today‚Äôs date { today_iso }.  
    All strategist reasoning must align strictly with this snapshot, treating it as the ground truth of the economy at time T.  
    If any discrepancy arises between other sources and Finnhub, Finnhub always prevails.

    ## INPUTS

    - Perplexity discovery: {state.get("articles")} 
    - Partner research: {json.dumps(state.get("abcg_research"), ensure_ascii=False)}
    - User query: { (state.get("question") or "").strip() }
    - Finnhub economic calendar : || {json.dumps(state.get("economic_calendar_agent"), ensure_ascii=False)} ||

    ---


    ## CRITICAL RULES

    - Market data ‚Üí ONLY Twelve Data (valid intervals only).  
    - Fundamentals ‚Üí ONLY ForexFactory/TradingEconomics.  
    - News ‚Üí ONLY original publishers.  
    - Ranges 3d/5d must always be computed locally from 1day data (never API calls).  
    - Missing sections or invalid JSON will invalidate your output.  

    Do not mention or reveal any third-party data vendors or platforms ( finnhub twelve, data, tradingview) when 
    attribution is needed, use generic phrasing while keeping the existing JSON structure unchanged 

    CRITICAL OUTPUT RULES:

    - Return RAW JSON only, no markdown, no quotes, UTF-8 safe
    - Do NOT wrap the JSON in markdown
    - Do NOT use ```json or ```
    - Do NOT add explanations or text before or after
    - The first character MUST be '{{'
    - The last character MUST be '}}'
    
    You MUST return valid JSON following EXACTLY this schema.
        that follows EXACTLY this schema:

        {parser.get_format_instructions()}
    Rules:
        - All percentages MUST be strings (e.g. "85%")
        - Never use trailing commas
        - Never include comments
        - Never include text outside JSON
    """

    user_prompt = f"""
    {(state["body"].get("question") or "").strip()}
    """

    try:
        stream = client.chat.completions.create(
            messages=[
                {
                    "role": "system",
                    "content": system_prompt
                },
                            {
                    "role": "user",
                    "content": user_prompt
                }
            ],
            model="llama-3.3-70b",
            stream=True,
            max_completion_tokens=65000,
            temperature=1,
            top_p=0.95
        )

        res = ""
        for chunk in stream:
            # print(chunk.choices[0].delta.content or "", end="")
            res += chunk.choices[0].delta.content or ""

    except Exception as e:
        error_reason = f"LLM invocation failed: {str(e)}"

    end = time.time()
    # print("remember user s question was :", (state["body"].get("question") or "").strip())
    print(f"Final Synthesis Agent completed in {end - start:.2f} seconds.")

    # -----------------------------
    # üõë CASE 1 ‚Äî Hard failure
    # -----------------------------
    if res is None:
        print("‚ö†Ô∏è Final agent LLM invocation failed")
        return {
            "output": {
                "final_answer": "Unavailable",
                "confidence_note": "LLM call failed",
                "error": error_reason,
            }
        }

    # -----------------------------
    # üõë CASE 2 ‚Äî Tool call only
    # -----------------------------

    # -----------------------------
    # üõë CASE 3 ‚Äî Empty content
    # -----------------------------
    content = (res or "").strip()
    if not content:
        print("‚ö†Ô∏è Empty LLM content")

        return {
            "output": {
                "final_answer": "Unavailable",
                "confidence_note": "Empty LLM response",
            }
        }

    # -----------------------------
    # ‚úÖ CASE 4 ‚Äî Normal path
    # -----------------------------

    return {
        "output": {
            "final_answer": content.strip(),
            "confidence_note": (
                "Partner research unavailable"
                if isinstance(state.get("abcg_research"), dict)
                and state["abcg_research"].get("status") == "unavailable"
                else "Partner research integrated"
            )
        }
    }

def final_synthesis_agent_dev(state: DirectionState) -> DirectionState:
    start = time.time()

    llm = ChatOpenAI(
        model="gpt-4.1-mini",
        temperature=0
    )

    resp = None
    error_reason = None

    system_prompt = f"""

Today‚Äôs date is: { today_iso } and You are a senior FX/macro strategist at a top-tier macro research firm with WEB BROWSING ENABLED.  

Your mandate is THREE-FOLD in a SINGLE OUTPUT:  

1. **Market Data Collection**  
    TOOL RESPONSE MESSAGE HISTORY:
    {state.get("messages")}
  ‚ö†Ô∏è Remember: **all this market data is contextual metadata. The only mandatory final output is the `content` field.**

2. **Strategist Report Construction (Weekly Outlook)**  
   - Use Partner institutional insights (ABCG RESEARCH) as foundation. Expand clearly.  
   - Complement with contextual news discovered via Perplexity, but cite only the original publishers.  
   - **Tone must be institutional, structured, confident (Goldman Sachs / JPMorgan style).**  
   - **Content base** ‚Üí Anchor in Partner Research. Expand clearly.  
   - **News complement** ‚Üí Enrich with news (discovered via Perplexity) but always cite only the original publishers.  

   - Mandatory structure:  

   ---

   Executive Summary  
   {{2‚Äì3 sentences}}  

   Fundamental Analysis  
   {{Narrative + bullets}}  

   Directional Bias  
   {{Bullish/Bearish/Neutral}} 
   Confidence: {{XX}}%  

   Key Levels  
   Support  
   {{level1}}  
   {{level2}}  
   Resistance  
   {{level1}}  
   {{level2}}  
  
   AI Insights Breakdown  
   Toggle GPT  
   {{GPT narrative}} 

   Toggle Curated  
   {{Institutional view}}  
   ---  

3. **Fundamentals Enrichment**  
   - All macroeconomic fundamentals (releases, actual, consensus, previous, timestamps) must come exclusively from the Finnhub Economic Calendar API.  
   - If an event is not present in Finnhub, mark it `"Unavailable"`.  
   - Perplexity or other news sources may only be used for qualitative context (commentary, sentiment), never for datapoints.
   - Always include: indicator, actual, consensus, previous, release timestamp, checked_at.  
   - If web access fails, add `"warning": "web access to FF/TE unavailable"`.  
   - **Enriched note must be the base_report enriched with:**  
     - inline numeric clarifications,  
     - appended sections: Fundamentals, Rate Differentials, Positioning, Balance of Payments, Central Bank Pricing, Sentiment Drivers, Event Watch.  
   - ‚ö†Ô∏è **This enriched strategist note must always be placed in the `content` field.  
     The `content` field is the one and only mandatory narrative output.  
     All other fields (request, market_data, meta, base_report, fundamentals, citations_news) are supporting context and traceability only.**

---

## OUTPUT RULES

- Produce a single JSON

‚ö†Ô∏è Important: The list of collected_intervals and series timeslots above is only an example.  
Always decide dynamically which intervals to collect depending on the user‚Äôs query and trading horizon.  
Do not fetch redundant data if it is not required.  

‚ö†Ô∏è Critical: The field `content` must always contain the **final enriched strategist note** and be at the VERY ROOT OF THE JSON OUTPUT.  
This is the **main output of your work**.  
All other fields are optional context, metadata, and traceability, but `content` is the only mandatory narrative output.  

Every field must exist in the JSON (if data is missing, use empty string, empty array, or `"Unavailable"`).  
Return only JSON, no free text.  

---
‚ö†Ô∏è Temporal Truth Rule:  
At any point in the reasoning, the authoritative state of macroeconomic fundamentals is the Finnhub Economic Calendar snapshot provided at runtime ( see below )  
This snapshot reflects the truth of the economic environment at today‚Äôs date { today_iso }.  
All strategist reasoning must align strictly with this snapshot, treating it as the ground truth of the economy at time T.  
If any discrepancy arises between other sources and Finnhub, Finnhub always prevails.

## INPUTS

- Perplexity discovery: {state.get("articles")} 
- Partner research: {json.dumps(state.get("abcg_research"), ensure_ascii=False)}
- User query: { (state["body"].get("question") or "").strip() }
- Finnhub economic calendar : || {json.dumps(state.get("economic_calendar_agent"), ensure_ascii=False)} ||

---


## CRITICAL RULES

- Market data ‚Üí ONLY Twelve Data (valid intervals only).  
- Fundamentals ‚Üí ONLY ForexFactory/TradingEconomics.  
- News ‚Üí ONLY original publishers.  
- Ranges 3d/5d must always be computed locally from 1day data (never API calls).  
- Missing sections or invalid JSON will invalidate your output.  

Do not mention or reveal any third-party data vendors or platforms ( finnhub twelve, data, tradingview) when 
attribution is needed, use generic phrasing while keeping the existing JSON structure unchanged 

"""

    user_prompt = f"""
{(state["body"].get("question") or "").strip()}
"""

    try:
        resp = llm.invoke([
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ])

    except Exception as e:
        error_reason = f"LLM invocation failed: {str(e)}"

    end = time.time()
    # print("remember user s question was :", (state["body"].get("question") or "").strip())
    print(f"Final Synthesis Agent completed in {end - start:.2f} seconds.")

    # -----------------------------
    # üõë CASE 1 ‚Äî Hard failure
    # -----------------------------
    if resp is None:
        print("‚ö†Ô∏è Final agent LLM invocation failed")
        return {
            "output": {
                "final_answer": "Unavailable",
                "confidence_note": "LLM call failed",
                "error": error_reason,
            }
        }

    # -----------------------------
    # üõë CASE 2 ‚Äî Tool call only
    # -----------------------------
    if getattr(resp, "tool_calls", None):
        print("‚ö†Ô∏è Final agent emitted tool calls instead of content")
        print("Tool calls:", resp.tool_calls)

        return {
            "output": {
                "final_answer": (
                    "Market data requested via tool call. "
                    "Final narrative could not be generated at this stage."
                ),
                "confidence_note": "Deferred ‚Äî awaiting tool execution",
                "tool_calls": resp.tool_calls,
            }
        }

    # -----------------------------
    # üõë CASE 3 ‚Äî Empty content
    # -----------------------------
    content = (resp.content or "").strip()
    if not content:
        print("‚ö†Ô∏è Empty LLM content")

        return {
            "output": {
                "final_answer": "Unavailable",
                "confidence_note": "Empty LLM response",
            }
        }

    # -----------------------------
    # ‚úÖ CASE 4 ‚Äî Normal path
    # -----------------------------

    return {
        "output": {
            "final_answer": content,
            "confidence_note": (
                "Partner research unavailable"
                if isinstance(state.get("abcg_research"), dict)
                and state["abcg_research"].get("status") == "unavailable"
                else "Partner research integrated"
            )
        }
    }

async def economic_calendar_agent(state: DirectionState) -> Dict[str, Any]:
    start = time.time()
    import httpx
    from datetime import datetime, timedelta, timezone
    import re

    QUESTION_RAW = (state["question"] or "").strip()
    QUESTION = QUESTION_RAW.upper()
    NOW = datetime.now(timezone.utc)

    # ------------------------------
    # Helpers: dates
    # ------------------------------
    def to_ymd(d: datetime) -> str:
        return d.strftime("%Y-%m-%d")

    def compute_range():
        from_d = NOW
        to_d = NOW

        if "NEXT WEEK" in QUESTION:
            from_d -= timedelta(days=7)
            to_d += timedelta(days=14)
        elif "LAST MONTH" in QUESTION:
            from_d -= timedelta(days=30)
        else:
            from_d -= timedelta(days=7)
            to_d += timedelta(days=7)

        return to_ymd(from_d), to_ymd(to_d)

    # ------------------------------
    # Detect countries
    # ------------------------------
    CCY_TO_COUNTRY = {
        "USD": "US", "EUR": "EU", "GBP": "UK", "JPY": "JP",
        "CHF": "CH", "AUD": "AU", "CAD": "CA", "NZD": "NZ", "CNY": "CN",
    }

    countries = set()

    for a, b in re.findall(r"([A-Z]{3})/([A-Z]{3})", QUESTION):
        if a in CCY_TO_COUNTRY: countries.add(CCY_TO_COUNTRY[a])
        if b in CCY_TO_COUNTRY: countries.add(CCY_TO_COUNTRY[b])

    for ccy, ctry in CCY_TO_COUNTRY.items():
        if ccy in QUESTION:
            countries.add(ctry)

    if not countries:
        countries = {"US", "EU"}

    # ------------------------------
    # Event filters
    # ------------------------------
    EVENT_MAP = {
        "CPI": ["CPI", "INFLATION"],
        "NFP": ["NONFARM", "PAYROLL", "NFP"],
        "GDP": ["GDP"],
        "PMI": ["PMI"],
        "RATES": ["RATE", "FOMC", "ECB", "BOE", "BOJ"],
        "JOBS": ["UNEMPLOY", "EMPLOY"],
    }

    requested = [
        k for k, pats in EVENT_MAP.items()
        if any(p in QUESTION for p in pats)
    ]

    def match_event(name: str) -> bool:
        if not requested:
            return True
        name = name.upper()
        return any(
            any(p in name for p in EVENT_MAP[k])
            for k in requested
        )

    def impact_allowed(impact: str) -> bool:
        if impact in ("high", "medium"):
            return True
        return impact == "low" and "LOW" in QUESTION

    def to_iso(ts: str):
        try:
            return datetime.fromisoformat(ts.replace(" ", "T") + "Z").isoformat()
        except Exception:
            return "Unavailable"

    def val(v):
        return "N/A" if v in (None, "", []) else str(v)

    from_d, to_d = compute_range()
    FINNHUB_URL = "https://finnhub.io/api/v1/calendar/economic"
    FINNHUB_TOKEN = os.environ["FINNHUB_TOKEN"]
    TOKEN = FINNHUB_TOKEN
    # TOKEN = state["body"].get(
    #     "finnhubToken",
    #     ""
    # )

    try:
        start = time.time()
        async with httpx.AsyncClient(timeout=30) as client:
            r = await client.get(
                FINNHUB_URL,
                params={"token": TOKEN, "from": from_d, "to": to_d},
            )
            r.raise_for_status()
            calendar = r.json().get("economicCalendar", [])

        filtered = [
            {
                "event": ev.get("event", "Unavailable"),
                "country": ev.get("country", "Unavailable"),
                "time": to_iso(ev.get("time")),
                "actual": val(ev.get("actual")),
                "consensus": val(ev.get("estimate")),
                "previous": val(ev.get("prev")),
                "impact": (ev.get("impact") or "Unavailable").lower(),
            }
            for ev in calendar
            if ev.get("country") in countries
            and impact_allowed((ev.get("impact") or "").lower())
            and match_event(ev.get("event", ""))
        ][:10]
        end = time.time()
        print(f"Economic Calendar Agent completed in {end - start:.2f} seconds.")
        return {
            "economic_calendar": {
                "economic_events": filtered or "Unavailable",
                "meta": {
                    "detected_countries": list(countries),
                    "requested_event_filters": requested or "ALL",
                    "date_range": {"from": from_d, "to": to_d},
                    "total_events_received": len(calendar),
                    "total_events_kept": len(filtered),
                },
            }
        }

    except Exception as e:
        end= time.time()
        print(f"Economic Calendar Agent failed in {end - start:.2f} seconds.")
        return {
            "economic_calendar": {
                "economic_events": "Unavailable",
                "meta": {
                    "error": {
                        "message": str(e),
                        "checked_at": datetime.utcnow().isoformat(),
                        "source": FINNHUB_URL,
                    }
                },
            }
        }

from langchain_core.messages import AIMessage, ToolMessage
import json

def extract_tool_outputs(state: DirectionState) -> dict:
    tool_call_id_to_name = {}
    forecast = None
    market = None
    print("extract has been called")

    # 1Ô∏è‚É£ r√©cup√©rer les tool_calls depuis les AIMessage
    for m in state["messages"]:
        if isinstance(m, AIMessage) and m.tool_calls:
            for tc in m.tool_calls:
                tool_call_id_to_name[tc["id"]] = tc["name"]

    # 2Ô∏è‚É£ lire les ToolMessage et matcher via tool_call_id
    for m in state["messages"]:
        if isinstance(m, ToolMessage):
            tool_name = tool_call_id_to_name.get(m.tool_call_id)

            if tool_name == "forecast_aws_api":
                forecast = json.loads(m.content)
                print("***********************")
                print(f"forecast_aws_api TOUCHE")
                print(forecast)
                print("***********************")

            elif tool_name == "twelve_data_time_series":
                market = json.loads(m.content)
                print("***********************")
                print(f"twelve_data_time_series TOUCHE")
                print("***********************")
    print(f"forecast data extracted: is BTC in {"BTC" in json.dumps(forecast, indent=2)}")
    return {
        "forecast_data": forecast,
        "market_data": market,
    }

def start_node(state: DirectionState) -> DirectionState:
    return {}

def start_trade_queued_node(state: DirectionState) -> DirectionState:
    return {}

def extract_first_json(text: str) -> dict:
    """
    Extract and parse the first valid JSON object from a string.
    Safe against:
    - extra text before/after
    - multiple concatenated JSON objects
    """
    decoder = json.JSONDecoder()
    text = text.strip()

    try:
        obj, idx = decoder.raw_decode(text)
        return obj
    except json.JSONDecodeError as e:
        raise ValueError(f"No valid JSON found: {e}") from e

def build_run_graph():
    graph = StateGraph(DirectionState)
    graph.add_node("start", start_node)
    graph.add_node("fanout", lambda s: s)
    graph.add_node("data_collection_llm", data_collection_llm_agent)
    graph.add_node("abcg_research", abcg_research_agent)
    graph.add_node("economic_calendar", economic_calendar_agent)
    graph.add_node("planner", planner_agent)
    graph.add_node("tools", ToolNode([twelve_data_time_series]))
    graph.add_node("final", final_synthesis_agent)

    # router
    graph.set_entry_point("start")
 
    graph.add_edge("start", "fanout")
    graph.add_edge("fanout", "data_collection_llm")
    graph.add_edge("fanout", "abcg_research")
    graph.add_edge("fanout", "economic_calendar")

    graph.add_edge("data_collection_llm", "planner")
    graph.add_edge("abcg_research", "planner")
    graph.add_edge("economic_calendar", "planner")
    graph.add_edge("planner", "tools")
    graph.add_edge("tools", "final")
    graph.add_edge("final", END)

    return graph.compile()

def build_trade_graph():
    graph = StateGraph(DirectionState)
    graph.add_node("start", start_trade_queued_node)
    graph.add_node("fanout", lambda s: s)
    graph.add_node("data_collection_llm", data_collection_llm_agent)
    graph.add_node("abcg_research", abcg_research_agent)
    graph.add_node("economic_calendar", economic_calendar_agent)
    graph.add_node("planner", planner_trade_agent)
    graph.add_node("tools", ToolNode([forecast_aws_api,twelve_data_time_series]))
    graph.add_node("extract_tool_outputs", extract_tool_outputs)
    graph.add_node("planner_trade_agent", planner_agent)
    graph.add_node("final", generate_trade_agent)    

    # router
    graph.set_entry_point("start")

    graph.add_edge("start", "fanout")
    graph.add_edge("fanout", "data_collection_llm")
    graph.add_edge("fanout", "abcg_research")
    graph.add_edge("fanout", "economic_calendar")

    graph.add_edge("data_collection_llm", "planner")
    graph.add_edge("abcg_research", "planner")
    graph.add_edge("economic_calendar", "planner")
    graph.add_edge("planner", "tools")
    graph.add_edge("tools", "extract_tool_outputs")
    graph.add_edge("extract_tool_outputs", "final")
    graph.add_edge("final", END)
    print("Trade graph built.")
    return graph.compile()

def make_supabase_update_node(table: str, fields: dict, filter_key: str):
    start = time.time()
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    def node(state: dict) -> dict:
        body = state.get("body", {})
        value = body.get(filter_key)

        if not value:
            raise ValueError(f"{filter_key} missing")

        supabase.table(table).update(fields).eq("id", value).execute()
        return state
    end = time.time()
    print(f"Supabase update node created in {end - start:.2f} seconds.")
    return node

def build_trade_graph():
    graph = StateGraph(DirectionState)

    # supabase_reading_news = make_supabase_update_node(
    #     table="jobs",
    #     fields={
    #         "status": "pending",
    #         "progress_message": "Reading the news"
    #     },
    #     filter_key="job_id"
    # )
    graph.add_node("start", start_trade_queued_node)
    graph.add_node("fanout", lambda s: s)
    graph.add_node("data_collection_llm", data_collection_llm_agent)
    graph.add_node("abcg_research", abcg_research_agent)
    graph.add_node("economic_calendar", economic_calendar_agent)
    graph.add_node("planner", planner_trade_agent)
    graph.add_node("tools", ToolNode([forecast_aws_api]))
    graph.add_node("final", generate_trade_agent)
    # graph.add_node("supabase_reading_news", supabase_reading_news)

    # router
    graph.set_entry_point("start")

    graph.add_edge("start", "fanout")
    graph.add_edge("fanout", "data_collection_llm")
    graph.add_edge("fanout", "abcg_research")
    graph.add_edge("fanout", "economic_calendar")

    graph.add_edge("data_collection_llm", "planner")
    graph.add_edge("abcg_research", "planner")
    graph.add_edge("economic_calendar", "planner")
    graph.add_edge("planner", "tools")
    graph.add_edge("tools", "final")
    graph.add_edge("final", END)

    return graph.compile()

def extract_first_json(text: str) -> dict:
    """
    Extract and parse the first valid JSON object from a string.
    Safe against:
    - extra text before/after
    - multiple concatenated JSON objects
    """
    decoder = json.JSONDecoder()
    text = text.strip()

    try:
        obj, idx = decoder.raw_decode(text)
        return obj
    except json.JSONDecodeError as e:
        raise ValueError(f"No valid JSON found: {e}") from e

@app.post("/run")
async def run_webhook(request: Request):
    """
    AWS Lambda entrypoint
    """
    try:

        start = time.time()
        body = await request.json()
        if isinstance(body, str):
            body = json.loads(body)

        # state = {
        #     "body": body,         
        #     "messages": [],
        #     "tradeMessages": [],
        #     "articles": [],
        #     "macro_insight": "",
        #     "abcg_research": None,
        #     "trade_setup": None,
        #     "economic_calendar": None,
        #     "market_data": None,
        #     "output": {},
        #     "error": None
        # }

        state = body

        _RUN_GRAPH = build_run_graph()
        _TRADE_GRAPH = build_trade_graph()

        # _direction_agent = get_direction_agent()

        if body.get("mode", "") == "trade_generation":
            result = await build_trade_graph().ainvoke(state)
        else:
            result = await build_run_graph().ainvoke(state)

        raw = result["output"]["final_answer"]

        clean = strip_json_fences(raw)
        # parsed = json.loads(clean)
        raw = extract_first_json(raw)
        print("Parsed LLM output:", raw)

        supabase_update_job_status(state,{ 
                "job_id" : "NONE",
                "message": {"content" : { "content": raw  }} }
                )
        
        # clean = strif_json_fences(raw)
        # parsed = json.loads(clean)

        end = time.time()
        print(f"Total Lambda execution completed in {end - start:.2f} seconds.")
        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json"},
            "body": { 
                "message" : { 
                "status" : "done",
                "job_id" : "NONE",
                "message": {"content" : { "content": raw}} }
                },
            "message": { 
                "message" : { 
                "status" : "done",
                "job_id" : "NONE",
                "message": {"content" : { "content": raw  }} }
                }
        }

    except Exception as e:
        return {
            "statusCode": 500,
            "body": json.dumps({
                "error": str(e)
            })
        }