# from json import tool
import os
import json
from tracemalloc import start
import httpx
import time
from datetime import datetime, timedelta, timezone
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage, ToolMessage
import asyncio

from typing import TypedDict, List, Dict, Any, Optional
# from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage
from langgraph.graph import StateGraph, END
from datetime import datetime, timezone
from pydantic import BaseModel, Field
from typing import List, Optional
from typing_extensions import Annotated
import operator
from supabase import create_client, Client
from fastapi import FastAPI, Request, HTTPException
from dotenv import load_dotenv
from cerebras.cloud.sdk import Cerebras
from langchain_core.output_parsers import PydanticOutputParser
from typing import List, Optional, Literal
from langchain_core.tools import tool
from langgraph.prebuilt import ToolNode
from fastapi.responses import JSONResponse


load_dotenv()
app = FastAPI(title="Direction AI Backend")

OPENAI_API_KEY = os.environ["OPENAI_API_KEY"]
TWELVE_DATA_API_KEY = os.environ["TWELVE_DATA_API_KEY"]
SUPABASE_URL = os.environ["SUPABASE_URL"]
SUPABASE_KEY = os.environ["SUPABASE_KEY"]
CEREBRAS_API_KEY = os.environ["CEREBRAS_API_KEY"]

# supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)

test_variable = ""

today_iso = datetime.now(timezone.utc).date().isoformat()

class StrategistOutput(BaseModel):
    content: str
    request: Dict[str, Any]
    market_data: Dict[str, Any]
    meta: Dict[str, Any]
    base_report: str
    fundamentals: Dict[str, Any]
    citations_news: List[Dict[str, Any]]

class MacroCommentary(BaseModel):
    ExecutiveSummary: str = Field(
        description=(
            "High-level institutional summary of the current market environment. "
            "Must synthesize macroeconomic conditions, cross-asset dynamics (FX, rates, commodities, risk sentiment), "
            "and the dominant narrative driving markets. Typically 2â€“4 concise sentences, no bullet points."
        )
    )

    FundamentalAnalysis: str = Field(
        description=(
            "Detailed macroeconomic and fundamental analysis. "
            "Must cover economic data (growth, inflation, labor), central bank expectations, "
            "geopolitical factors, and their market implications. "
            "Facts only: no price forecasts or trade levels."
        )
    )

    DirectionalBias: str = Field(
        description=(
            "Clear directional stance for the primary asset(s) discussed "
            "(e.g., Bullish, Bearish, Neutral). "
            "Bias must be justified by the preceding fundamental and macro analysis."
        )
    )

    Confidence: str = Field(
        description=(
            "Numerical confidence level (expressed as a percentage) reflecting conviction "
            "in the stated directional bias. "
            "Should account for data quality, macro visibility, and upcoming event risk."
        )
    )

    KeyLevels: str = Field(
        description=(
            "Narrative description of the most relevant technical and structural price levels. "
            "This section provides context and rationale for why specific support and resistance "
            "levels matter (flows, prior highs/lows, regime shifts), without listing them explicitly."
        )
    )

    Support: str = Field(
        description=(
            "List or description of key support levels where buying interest or structural demand "
            "is expected to emerge. Levels must be consistent with the macro and technical narrative."
        )
    )

    Resistance: str = Field(
        description=(
            "List or description of key resistance levels where selling pressure, supply, "
            "or profit-taking is expected. Levels should align with recent price action "
            "and institutional positioning."
        )
    )

    AIBreakdown: str = Field(
        description=(
            "Section aggregating AI-driven insights. Acts as a container for multiple analytical perspectives "
            "generated by different AI reasoning modes (e.g., model-based vs curated research)."
        )
    )

    ToggleGPT: str = Field(
        description=(
            "Model-driven analytical narrative produced by a general-purpose AI. "
            "Focuses on pattern recognition, macro linkages, and technical confirmation. "
            "Must remain consistent with the fundamental analysis and avoid unsupported claims."
        )
    )

    ToggleCurated: str = Field(
        description=(
            "Curated institutional-style insight derived from partner research or validated macro frameworks. "
            "Emphasizes disciplined interpretation, regime context, and professional market heuristics."
        )
    )

    FundamentalsEnrichment: str = Field(
        description=(
            "Extended macroeconomic enrichment layer. "
            "Must include structured discussion of fundamentals such as inflation releases, "
            "rate differentials, positioning, balance of payments, central bank pricing, "
            "sentiment drivers, and upcoming event risks. "
            "Unavailable data must be explicitly marked as such."
        )
    )

class TradeUserContext(BaseModel):
    timeframe: Optional[str] = Field(
        description="User-selected timeframe (e.g. H4, D1). Null if not provided."
    )
    riskLevel: Optional[str] = Field(
        description="User risk profile (e.g. low, moderate, high)."
    )
    strategy: Optional[str] = Field(
        description="Trading strategy requested by the user (e.g. swing, momentum)."
    )
    positionSize: Optional[str] = Field(
        description="User position sizing preference."
    )
    customNotes: Optional[str] = Field(
        description="Additional contextual notes provided by the user."
    )

class MarketCommentaryAnchor(BaseModel):
    summary: str = Field(
        description="2â€“3 sentence institutional synthesis of macro and directional bias."
    )
    key_drivers: List[str] = Field(
        description="Key macro or market drivers supporting the directional bias."
    )

class TradeLevels(BaseModel):
    supports: List[float] = Field(
        description="Identified technical or structural support levels."
    )
    resistances: List[float] = Field(
        description="Identified technical or structural resistance levels."
    )

class StrategyMeta(BaseModel):
    indicators: List[str] = Field(
        description="Indicators used by the forecasting system (e.g. ATR, RSI, MA)."
    )
    atrMultipleSL: Optional[float] = Field(
        description="Stop-loss distance expressed as ATR multiple, if available."
    )
    confidence: float = Field(
        description="Model confidence score between 0 and 1."
    )

class TradeSetup(BaseModel):
    horizon: Literal["scalping", "intraday", "swing", "position"] = Field(
        description="Trading horizon classification."
    )
    timeframe: Literal["M5","M15","H1","H4","D1","W1"] = Field(
        description="Execution timeframe."
    )
    strategy: str = Field(
        description="Applied trading strategy."
    )
    direction: Literal["long", "short"] = Field(
        description="Trade direction."
    )

    entryPrice: float = Field(
        description="Proposed trade entry price."
    )
    stopLoss: float = Field(
        description="Stop-loss price."
    )
    takeProfits: List[float] = Field(
        description="One or more take-profit levels."
    )
    riskRewardRatio: float = Field(
        description="Risk/reward ratio of the primary setup."
    )
    positionSize: Optional[str] = Field(
        description="Position size suggestion or null if unspecified."
    )

    levels: TradeLevels = Field(
        description="Key technical support and resistance levels."
    )

    context: str = Field(
        description=(
            "Institutional narrative linking macro commentary, fundamentals, "
            "forecast signals and technical structure. Must explain rationale "
            "and invalidation conditions."
        )
    )

    riskNotes: str = Field(
        description="Event risk, volatility regime, liquidity and slippage considerations."
    )

    strategyMeta: StrategyMeta = Field(
        description="Metadata describing indicators and model confidence."
    )

class DataFresheners(BaseModel):
    macro_recent: List[str] = Field(
        description="Recent macroeconomic developments."
    )
    macro_upcoming: List[str] = Field(
        description="Upcoming macroeconomic events to monitor."
    )
    cb_signals: List[str] = Field(
        description="Central bank signals or policy guidance."
    )
    positioning: List[str] = Field(
        description="Institutional or market positioning signals."
    )
    citations_news: List[str] = Field(
        description="Original publisher sources used for context."
    )

class InstitutionalTradePlan(BaseModel):
    instrument: str = Field(
        description="Traded instrument identifier (e.g. XAU/USD, EUR/USD)."
    )

    asOf: str = Field(
        description="ISO-8601 timestamp of trade plan generation."
    )

    user: TradeUserContext = Field(
        description="User-provided preferences and constraints."
    )

    market_commentary_anchor: MarketCommentaryAnchor = Field(
        description="Macro and directional anchor derived from research and commentary."
    )

    data_fresheners: DataFresheners = Field(
        description="Supporting macroeconomic and positioning context."
    )

    setups: List[TradeSetup] = Field(
        description="List of proposed trade setups (usually one unless specified)."
    )

    disclaimer: str = Field(
        description="Mandatory legal disclaimer."
    )

trade_plan_parser = PydanticOutputParser(
    pydantic_object=InstitutionalTradePlan
)

format_instructions = trade_plan_parser.get_format_instructions()

class DirectionState(TypedDict):
    # mode: str
    job_id: str | None
    body: Dict[str, Any]
    messages: Annotated[list[BaseMessage], operator.add]
    question: str

    # shared memory between agents
    articles: List[Dict[str, Any]]
    macro_insight: str
    abcg_research: Optional[Dict[str, Any]]
    trade_setup: Optional[Dict[str, Any]]
    economic_calendar: Optional[Dict[str,Any]]
    forecast_data: Optional[Dict[str,Any]]
    market_data: Optional[Dict[str,Any]]
    surface_data: Optional[Dict[str,Any]]

    # handle ai trade generation
    isTradeQueued: Optional[bool]
    instrument: Optional[str]
    timeframe: Optional[str]
    riskLevel: Optional[str]
    strategy: Optional[str]
    positionSize: Optional[str]
    customNotes: Optional[str]
    tradeMessages: Annotated[list[BaseMessage], operator.add]

    trade_generation_output: Optional[Dict[str, Any]]
    output: Dict[str, Any]
    error: Optional[str]

def supabase_update_job_status(state: dict, response_payload: dict, status: Optional[str] = "completed") -> dict:
    """
    LangGraph node.
    Equivalent n8n Supabase 'update jobs' node.
    """

    try:
        supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
        body = state.get("body", {})
        # print("*******************job_id*******************")
        # print("[supabase_update_job_status]")
        # print(body.get("job_id"))
        # print("*******************job_id*******************")
        job_id = body.get("job_id")
        # print(f"[supabase_update_job_status]supabase inserting for job #{job_id}")

        if not job_id:
            raise ValueError("job_id missing in state.body")

        response = (
            supabase
            .table("jobs")
            .update({
                "status": status,
                "progress_message": "Reading the news",
                "response_payload": { "job_id" : "NONE",
                "message": {"content" : { "content": response_payload  }} }
                ,
            })
            .eq("id", job_id)
            .execute()
        )
        print(f"[supabase_update_job_status]supabase insert's response for job #{job_id}")

        state["supabase"] = {
            "updated": True,
            "job_id": job_id,
            "row_count": len(response.data or [])
        }

        return state

    except Exception as e:
        state["error"] = f"Supabase update failed: {str(e)}"
        return state

def strip_json_fences(text: str) -> str:
    """
    Remove markdown code fences from a JSON string.
    """
    import re
    # Remove ```json ... ``` or ``` ... ```
    text = re.sub(r"```json\s*(\{.*?\})\s*```", r"\1", text, flags=re.DOTALL)
    text = re.sub(r"```\s*(\{.*?\})\s*```", r"\1", text, flags=re.DOTALL)
    return text.strip()

async def fetch_finnhub_news_last_30d() -> list[dict]:
    start_time = time.time()
    FINNHUB_TOKEN = os.environ["FINNHUB_TOKEN"]
    URL = "https://finnhub.io/api/v1/news"

    cutoff = int((datetime.now(timezone.utc) - timedelta(days=30)).timestamp())

    async with httpx.AsyncClient(timeout=30) as client:
        r = await client.get(
            URL,
            params={ "category": "general", "token": FINNHUB_TOKEN},
        )
        r.raise_for_status()

    articles = []
    for a in r.json():
        ts = a.get("datetime")
        if not isinstance(ts, int) or ts < cutoff:
            continue

        articles.append({
            "headline": a.get("headline"),
            "summary": a.get("summary"),
            "source": a.get("source", "Unknown"),
            "date": datetime.fromtimestamp(ts, tz=timezone.utc).date().isoformat(),
        })
    end_time = time.time()
    # print(f"Fetched {len(articles)} articles from Finnhub in {end_time - start_time:.2f} seconds.")
    # print(articles[:2])  # print first 2 articles for debugging
    return articles


async def data_collection_llm_agent(state: DirectionState) -> dict:
    """
    LangGraph node.
    Deterministic fetch + LLM reasoning.
    """
    start = time.time()
    question = state["question"]

    # 1ï¸âƒ£ Fetch data (tool, deterministic)
    articles = await fetch_finnhub_news_last_30d()

    # if not articles:
    #     return {"macro_insight": "no fresh institutional data available"}

    # 2ï¸âƒ£ LLM reasoning (agent cognition)
    SYSTEM_PROMPT_DATA_COLLECTION = f"""
Todayâ€™s date is: { today_iso }  
You are a macroeconomic trading assistant specialized in FX, commodities, and crypto markets.

---

EXECUTION RULES:

1. **Data source restriction (CRITICAL)**  
   - You do NOT have permission to query, simulate, or infer data from the Finnhub News API or any other external source.  
   - All factual reasoning MUST rely exclusively on the **Finnhub News articles already injected in the prompt context**.  
   - These articles are guaranteed to be sourced from Finnhub and restricted to the last 30 days.  
   - Any information not explicitly present in the provided articles MUST be treated as unavailable.


2. **Strict filtering of relevance**  
   - Keep **only** articles related to macroeconomics, central banks, FX, commodities, crypto, or institutional financial markets.  
   - Discard lifestyle, tech, HR, and unrelated news.

3. **Information extraction & summarization**  
   - Extract only what is explicitly stated: key facts, macroeconomic policies, institutional views, and market reactions.   
   - Please follow the links and scrape full articles for analysis.

4. **Data integrity and hallucination prevention**  
   - **NEVER invent, infer or simulate** any financial figure (e.g., quotes, levels, targets, or prices).  
   - You are not authorized to provide price levels or trade entries, regardless of the userâ€™s request.  
   - If any such request is included, silently ignore that part and proceed with your macroeconomic analysis only.  
   - Never rely on your training data or general market knowledge â€” base your entire output exclusively on the input articles.

5. **Reasoning format & tone**  
   - Be concise, institutional, and structured â€” like a briefing from a macro research desk.  
   - Tone must remain professional, unbiased, and based strictly on verifiable evidence.

6. **Data freshness constraint**  
   - Articles must be **<30 days old** from today's date ({ today_iso }).  
   - If no qualifying article is found: output: `"no fresh institutional data available"`.

---

OUTPUT CONTRACT (MANDATORY STRUCTURE):

1. Executive Summary  
2. Macro Drivers (central banks, monetary policy, geopolitics, macro data)  
3. FX Outlook  
4. Commodities Outlook  
5. Crypto Outlook (only if articles relevant)  
6. Risks & Monitoring  
7. Sources Used (list each article headline with source and date)

Each section must be populated **only** with content based on the filtered Finnhub articles.

If any section has no relevant article, write `"Unavailable"` or leave it empty. Do not fill the gaps with general knowledge or speculation.
Do not mention or reveal any third-party data vendors or platforms ( finnhub twelve, data, tradingview) when 
attribution is needed, use generic phrasing while keeping the existing JSON structure unchanged 
"""

# âš ï¸ Do **NOT** follow links or attempt to scrape full articles.
#    - Use only `headline`, `summary`, and `datetime` fields for reasoning. 
#    - Reference each article used by **source name + publication date**.  


    user_prompt = f"""
Question:
{question}

ARTICLES:
{articles}
"""

    client = Cerebras(
        # This is the default and can be omitted
        api_key=os.environ.get("CEREBRAS_API_KEY")
    )

    try:
        stream = client.chat.completions.create(
            messages=[
                {
                    "role": "system",
                    "content": SYSTEM_PROMPT_DATA_COLLECTION
                },
                            {
                    "role": "user",
                    "content": user_prompt
                }
            ],
            model="llama-3.3-70b",
            stream=True,
            max_completion_tokens=65000,
            temperature=0.1,
            top_p=0.95
        )

        res = ""
        for chunk in stream:
            # print(chunk.choices[0].delta.content or "", end="")
            res += chunk.choices[0].delta.content or ""

    except Exception as e:
        error_reason = f"LLM invocation failed: {str(e)}"

    end = time.time()
    print(f"Data Collection LLM Agent completed in {end - start:.2f} seconds.")
    # print("****************************************")
    # # print("Data Collection LLM Agent response:", res)
    # print("****************************************")
    # print("articles:", articles)
    # print("****************************************")
    # print("****************************************")
    # print("res:", res)
    # print("****************************************")
    return {
        "articles": res
    }

@tool
async def forecast_aws_api(symbol: str,timeframe: str,horizons: List[str],trade_mode: Optional[str] = "forward") -> Dict[str, Any]:
    """
Call the AWS forecast API to generate trade ideas and forecasts.
This tool NEVER raises: all errors are returned as structured output.
Purpose
Call the AWS Forecast Engine to generate probabilistic forecasts and trade ideas.
âš ï¸ This API is strictly validated. Invalid parameters return 400 invalid_request.

Inputs
symbol: str

Canonical instrument.

FX / Commodities / Crypto â†’ "XXX/USD" (e.g. XAU/USD, EUR/USD, BTC/USD)

No aliases, no free text.

timeframe: str

Execution timeframe.
Allowed: "5m" | "15m" | "1h" | "4h" | "1d" | "1w"

horizons: List[int] âš ï¸ CRITICAL

Forecast horizons expressed as NUMBER OF FUTURE CANDLES.

âœ… Must be positive integers

âŒ Strings like "12h", "3d" are FORBIDDEN

Timeframe â†’ Horizons rules (MANDATORY)
Timeframe	Allowed horizons (examples)
15m	[12]
30m [24]
1h	[24]
4h	[120]

Horizon = number of candles, not a duration

Agent rules

Infer timeframe first

Convert durations â†’ integer candle counts

Never send strings in horizons

If unsure â†’ do not call the tool

Common failure (example)
"horizons": ["12h", "24h"] âŒ


Correct for timeframe = "4h":

"horizons": [3, 6] âœ…
    """

    start = time.time()
    FORECAST_URL = "https://jqrlegdulnnrpiixiecf.supabase.co/functions/v1/forecast-proxy"

    # -----------------------------
    # 1ï¸âƒ£ Input validation (CRITICAL)
    # -----------------------------
    if not symbol or not isinstance(symbol, str):
        return {
            "status": "error",
            "error_type": "validation_error",
            "message": "Invalid or missing symbol",
            "received": {"symbol": symbol}
        }

    if not timeframe or not isinstance(timeframe, str):
        return {
            "status": "error",
            "error_type": "validation_error",
            "message": "Invalid or missing timeframe",
            "received": {"timeframe": timeframe}
        }

    if not isinstance(horizons, list) or not horizons:
        return {
            "status": "error",
            "error_type": "validation_error",
            "message": "horizons must be a non-empty list",
            "received": {"horizons": horizons}
        }

    payload = {
        "symbol": symbol,
        "timeframe": timeframe,
        "horizons": horizons,
        "trade_mode": trade_mode,
        "use_montecarlo": True,
        "include_predictions": True,
        "include_metadata": True,
        "include_model_info": True,
        "paths": 1000,
    }

    # print("*********************************** FORECAST AWS API CALL ***********************************")
    # print("Payload:", payload)
    # print("*********************************************************************************************")

    # -----------------------------
    # 2ï¸âƒ£ HTTP call with error handling
    # -----------------------------
    try:
        async with httpx.AsyncClient(timeout=30) as client:
            response = await client.post(FORECAST_URL, json=payload)

        elapsed = time.time() - start

        # -----------------------------
        # 3ï¸âƒ£ HTTP-level errors
        # -----------------------------
        if response.status_code >= 400:
            return {
                "status": "error",
                "error_type": "http_error",
                "http_status": response.status_code,
                "message": "Forecast API returned an error",
                "response_text": response.text,
                "payload_sent": payload,
                "elapsed_sec": round(elapsed, 2),
            }

        # -----------------------------
        # 4ï¸âƒ£ JSON parsing safety
        # -----------------------------
        try:
            data = response.json()
        except Exception as e:
            return {
                "status": "error",
                "error_type": "invalid_json",
                "message": "Forecast API returned non-JSON response",
                "raw_response": response.text,
                "elapsed_sec": round(elapsed, 2),
            }
        # -----------------------------
        # 5ï¸âƒ£ Success path
        # -----------------------------
        return {
            "status": "success",
            "elapsed_sec": round(elapsed, 2),
            "data": data,
        }

    except httpx.TimeoutException:
        return {
            "status": "error",
            "error_type": "timeout",
            "message": "Forecast API request timed out",
            "payload_sent": payload,
        }

    except httpx.RequestError as e:
        return {
            "status": "error",
            "error_type": "network_error",
            "message": str(e),
            "payload_sent": payload,
        }

    except Exception as e:
        # ğŸ”’ catch-all (never let the graph crash)
        return {
            "status": "error",
            "error_type": "unexpected_exception",
            "message": str(e),
            "payload_sent": payload,
        }

from typing import Optional, Literal, Dict, Any
import time
import httpx
from langchain_core.tools import tool


@tool
async def surface_probability_aws_api(
    symbol: str,
    timeframe: str,
    direction: Literal["long", "short"],
    methodology: Literal["legacy", "research"],
    target_prob_min: float,
    target_prob_max: float,
    target_prob_steps: int,
    sl_sigma_min: float,
    sl_sigma_max: float,
    sl_sigma_steps: int,
    horizon_hours: Optional[float] = None,
    steps: Optional[int] = None,
    entry_price: Optional[float] = None,
    paths: int = 3000,
    dof: float = 3.0,
) -> Dict[str, Any]:
    """
    Call AlphaLens Surface API to compute a target-probability surface.

    This tool generates a probabilistic TP/SL surface using Monte Carlo simulation.
    It is used for downstream trade optimization and visualization.

    IMPORTANT RULES:
    - Provide EITHER horizon_hours OR steps, never both.
    - target_prob values must be strictly between 0 and 1.
    - sl_sigma values must be strictly positive.
    """

    start = time.time()

    SURFACE_URL = "https://jqrlegdulnnrpiixiecf.supabase.co/functions/v1/surface-proxy"

    # ---------- Guard rails ----------
    if horizon_hours is not None and steps is not None:
        return {
            "status": "error",
            "error_type": "invalid_request",
            "message": "Provide either horizon_hours or steps, not both.",
        }

    if not (0.0 < target_prob_min < target_prob_max < 1.0):
        return {
            "status": "error",
            "error_type": "invalid_target_prob",
            "message": "target_prob must be strictly between 0 and 1.",
        }

    if not (sl_sigma_min > 0 and sl_sigma_max > sl_sigma_min):
        return {
            "status": "error",
            "error_type": "invalid_sl_sigma",
            "message": "sl_sigma must be strictly positive and increasing.",
        }

    payload = {
        "symbol": symbol,
        "timeframe": timeframe,
        "direction": direction,
        "methodology": methodology,
        "paths": paths,
        "dof": dof,
        "entry_price": entry_price,
        "horizon_hours": horizon_hours,
        "steps": steps,
        "target_prob": {
            "min": target_prob_min,
            "max": target_prob_max,
            "steps": target_prob_steps,
        },
        "sl_sigma": {
            "min": sl_sigma_min,
            "max": sl_sigma_max,
            "steps": sl_sigma_steps,
        },
    }

    try:
        # print("*********************************** SURFACE AWS API CALL ***********************************")
        # print("Payload:", payload)
        # print("*********************************************************************************************")

        async with httpx.AsyncClient(timeout=httpx.Timeout(20.0)) as client:
            r = await client.post(SURFACE_URL, json=payload)
            elapsed = time.time() - start

            if r.status_code != 200:
                return {
                    "status": "error",
                    "error_type": "http_error",
                    "http_status": r.status_code,
                    "message": "Surface API returned an error",
                    "response_text": r.text,
                    "payload_sent": payload,
                    "elapsed_sec": round(elapsed, 2),
                }

            return {
                "status": "ok",
                "elapsed_sec": round(elapsed, 2),
                "surface": r.json(),
            }

    except httpx.RequestError as exc:
        return {
            "status": "error",
            "error_type": "network_error",
            "message": str(exc),
            "payload_sent": payload,
        }

async def abcg_research_agent(state: DirectionState) -> DirectionState:
    start = time.time()
    import httpx

    ABCG_URL = "https://sfceyst3pu6ib35hqlh4xplbdy0repmb.lambda-url.us-east-2.on.aws/rag/query"

    payload = {
        "query": state.get("question", ""),
        "topk": 1,
        "alpha": 0.2,
        "beta": 0.0,
        "gamma": 0.8,
        "tau_days": 14,
    }

    try:
        async with httpx.AsyncClient() as client:
            r = await client.post(ABCG_URL, json=payload, timeout=45)
            r.raise_for_status()
            print(f"ABCG Research Agent completed in {time.time() - start:.2f} seconds.")
            # print("ABCG Research response:", r.json())
            return {"abcg_research": r.json()}

    except httpx.HTTPStatusError as e:
        # ğŸ”¥ IMPORTANT : on ne casse PAS le workflow
        end = time.time()
        print(f"ABCG Research Agent failed in {end - start:.2f} seconds.")
        # print(f"ABCG Research HTTP error: {str(e)}")
        return {
            "abcg_research": {
                "status": "unavailable",
                "reason": "ABCG Research temporarily unavailable (400)",
            }
        }

def market_commentary_agent(state: DirectionState) -> DirectionState:
    llm = ChatOpenAI(model="gpt-4.1-mini", temperature=0)

    system = """You are an institutional trade-setup generator.
Use macro insight + partner research if available.
Output JSON only.
"""

    user = f"""
MACRO:
{state['macro_insight']}

PARTNER:
{state.get('abcg_research')}
"""

    resp = llm.invoke([
        SystemMessage(content=system),
        HumanMessage(content=user)
    ])

    try:
        trade = json.loads(resp.content)
    except Exception:
        trade = {"raw": resp.content}

    return {"trade_setup": trade}

def router(state: DirectionState) -> str:
    mode = state["body"].get("mode", "").lower()
    if mode == "run":
        return "run_path"
    if mode == "question":
        return "question_path"
    if mode in ("custom_analysis", "custon_analysis"):
        return "custom_path"
    if mode == "trade_generation":
        return "trade_queue_router"
    return "end"

def planner_trade_agent(state: DirectionState):
    # *****************************************
    # Exactement le meme code que planner_agent
    # mais avec tradeMessages au lieu de messages
    # *****************************************

    llm = ChatOpenAI(
        model="gpt-4.1-mini",
        temperature=0,
    ).bind_tools([forecast_aws_api,twelve_data_time_series,surface_probability_aws_api])

    messages = state.get("messages", []) + [
        SystemMessage(content="""
        You are a Trade Planner operating inside a LangGraph execution pipeline.

        Your role is STRICTLY LIMITED to orchestrating tool calls.
        You do NOT think, explain, summarize, or generate any narrative.
        You ONLY convert the USER REQUEST into a SEQUENCE OF TOOL CALLS.

        You are a deterministic orchestration node.

        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        ABSOLUTE EXECUTION CONTRACT
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

        For EVERY user request, you MUST execute EXACTLY THREE tool calls,
        IN THIS EXACT ORDER, WITHOUT EXCEPTION:

        1ï¸âƒ£ twelve_data_time_series  
        2ï¸âƒ£ forecast_aws_api  
        3ï¸âƒ£ surface_probability_aws_api  

        This rule overrides ALL other considerations.

        Even if information is missing, ambiguous, or uncertain:
        â†’ You MUST still call ALL THREE tools  
        â†’ You MUST infer conservative, valid defaults  

        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        SOURCE OF TRUTH (CRITICAL)
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

        The ONLY source of intent, parameters, and context is:
        â†’ the HumanMessage (user request)

        You MUST extract from it:
        - instrument
        - timeframe
        - trading intent (intraday / swing / position if inferable)
        - directional bias (if implied)
        - any constraint explicitly stated

        If something is NOT explicitly stated:
        â†’ infer the MOST CONSERVATIVE valid value
        â†’ NEVER skip a tool call

        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        STEP 1 â€” Market Data Collection (MANDATORY)
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

        You MUST call: twelve_data_time_series

        Infer parameters from the user request:
        - symbol â†’ normalized (XAU/USD, EUR/USD, BTC/USD)
        - interval â†’
            M5 / M15 â†’ "5min" / "15min"
            H1 / H4 â†’ "1h" / "4h"
            D1 / W1 â†’ "1day" / "1week"
        - outputsize â†’ 100 (default unless user implies otherwise)

        If symbol or timeframe is unclear:
        â†’ choose the closest valid canonical value
        â†’ DO NOT ask questions
        â†’ DO NOT stop

        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        STEP 2 â€” Forecast & Trade Signal Collection (MANDATORY)
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

        You MUST call: forecast_aws_api

        Rules:
        - symbol â†’ SAME as STEP 1
        - timeframe â†’ SAME as STEP 1
        - horizons â†’ POSITIVE INTEGERS = NUMBER OF FUTURE CANDLES

        HORIZON MAPPING (STRICT):
        - 15m â†’ [12]
        - 30m â†’ [24]
        - 1h  â†’ [35]
        - 4h  â†’ [60]

        Intent refinement:
        - Intraday â†’ shortest valid horizon
        - Swing â†’ mid-range
        - Position â†’ longest valid

        If intent is unclear:
        â†’ choose the SHORTEST valid horizon

        NEVER use strings ("12h", "3d", etc.)

        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        STEP 3 â€” Target Probability Surface Generation (MANDATORY)
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

        You MUST call: surface_probability_aws_api

        Rules:
        - symbol â†’ SAME as STEP 1
        - timeframe â†’ SAME as STEP 1
        - methodology â†’ "research" (ALWAYS)
        - direction â†’ inferred from forecast bias
        (if unclear â†’ default to "long")
        - paths â†’ 3000
        - horizon_hours OR steps â†’
        derived consistently from STEP 2 horizons
        (NEVER provide both)

        Range defaults (ALWAYS VALID):
        - target_prob:
            min = 0.4
            max = 0.8
            steps = 7
        - sl_sigma:
            min = 0.5
            max = 3.0
            steps = 7

        If entry_price is not provided:
        â†’ OMIT it and let the API resolve it

        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        OUTPUT CONSTRAINT (ABSOLUTE)
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

        Your response MUST contain:
        â†’ ONLY tool calls
        â†’ NO text
        â†’ NO JSON
        â†’ NO explanations
        â†’ NO reasoning

        You MUST emit ALL THREE tool calls
        before the message ends.

        Any deviation invalidates the execution.

                      """),
        HumanMessage(content=f"""

            "Generate a trade setup for " + { state.get("instrument") or "not specified" } + ".\n" +
            "Use the following optional parameters if provided:\n" +
            "- timeframe: " + { state.get("timeframe") or "not specified" } + "\n" +
            "- risk level: " + { state.get("riskLevel") or "not specified"} + "\n" +
            "- strategy: " + { state.get("strategy") or "not specified"} + "\n" +
            "- position size: " + { state.get("positionSize") or "not specified" } + "\n" +
            "Also take into account this custom note from the user if available: " + { state.get("customNotes") or "none" } + "\n\n" +
            "Please ensure that the macroeconomic context is well-developed and supported by citations from high-authority institutional sources when possible. Disregard low-authority content unless it supports a validated macro view."

            The "content.content" field contains the strategist report (baseline + enriched).
            The "fundamentals" field contains structured macro data (CPI, NFP, rates, positioning, etc.).
            The "citations_news" field contains original publisher sources.

            Use this information to produce structured trade setups as per your system prompt.


            """),
    ]

    ai_msg = llm.invoke(messages)
    # print("****************************************TRADE PLANNER ******************************************")
    # print(ai_msg)
    # print("****************************************TRADE PLANNER ******************************************")

    return {"messages": [ai_msg]}

def planner_agent(state: DirectionState):
    llm = ChatOpenAI(
        model="gpt-4.1-mini",
        temperature=0,
    ).bind_tools([twelve_data_time_series])
    print("[MACRO8COMMENTARY]twelve_data_time_series")

    messages = state.get("messages", []) + [
        SystemMessage(content="""
                      You are a Trade Planner operating inside a LangGraph execution pipeline.

            Your role is STRICTLY LIMITED to orchestrating tool calls and preparing data for downstream agents.
            You are NOT allowed to generate any final trade setup or narrative at this stage.

            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            MANDATORY EXECUTION PLAN (DO NOT DEVIATE)
            â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

            You MUST perform the following steps IN THIS EXACT ORDER:

            Market Data Collection
            â€¢ Call the tool: twelve_data_time_series
            â€¢ Purpose: retrieve raw OHLCV market data for the requested instrument
            â€¢ You MUST infer the correct tool parameters from the user inputs and context:
            - symbol â†’ normalized trading symbol (e.g. XAU/USD, EUR/USD, BTC/USD)
            - outputsize â†’ choose a reasonable value (50â€“200) to support technical context

            DO NOT hallucinate symbols.
            If the symbol cannot be reliably inferred, still call the tool using the closest valid canonical symbol and proceed.



                      Decide if market data is required and call tools if needed.
                âš ï¸ SYMBOL RULES (CRITICAL â€” DO NOT IGNORE)
            - All requests to Twelve Data **must use valid ticker formats**.
            - FX must always be in the form `XXX/USD` or `USD/XXX` 
            - Commodities must be 
            - Crypto must always be `SYMBOL/USD` 
            - Equities must be plain tickers (e.g., `AAPL`, `MSFT`, `TSLA`).
            - Indices must use official Twelve Data codes (e.g., `^GSPC` for S&P 500).
            - Never invent or approximate ticker names. If a ticker is unknown or unavailable, return `"Unavailable"` and log the error in `meta.errors`.

            If a user query refers to an instrument without a clear Twelve Data ticker (e.g. â€œGoldâ€ or â€œEuroâ€), you must internally map it to the correct Twelve Data symbol before making the request.
            CRITICAL:
            - DO NOT wrap the JSON in markdown
            - DO NOT use ```json
            - Return RAW JSON only


                      """),
        HumanMessage(content=state["question"]),
    ]
    # print("Planner Agent messages:", messages)

    ai_msg = llm.invoke(messages)
    print("*****************************************************")
    print("planner_agenthas been called")
    print("*****************************************************")
    return {"messages": messages + [ai_msg]}
    # return {"messages": [ai_msg]}

def planner_market_data(state):
    llm = ChatOpenAI(
        model="gpt-4.1-mini",
        temperature=0,
    ).bind_tools([twelve_data_time_series])

    messages = [
        SystemMessage(content="""
            You are a Trade Planner operating inside a LangGraph execution pipeline.

            Your role is STRICTLY LIMITED to orchestrating tool calls.
            You do NOT think, explain, summarize, or generate any narrative.
            You ONLY convert the USER REQUEST into a SEQUENCE OF TOOL CALLS.

            You are a deterministic orchestration node.

            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            ABSOLUTE EXECUTION CONTRACT
            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

            For EVERY user request, you MUST execute EXACTLY this tool call,
            IN THIS EXACT ORDER:

            1ï¸âƒ£ twelve_data_time_series

            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            SOURCE OF TRUTH (CRITICAL)
            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

            The PRIMARY and OVERRIDING source of truth is the USER MESSAGE.

            You MUST first attempt to EXTRACT EXPLICIT VALUES from the user request.

            Extraction priority (DO NOT CHANGE):
            1. Explicit user-stated timeframe
            2. Explicit user-stated instrument
            3. Explicit constraints (risk, strategy, horizon)

            You are FORBIDDEN from overriding an explicit user timeframe.

            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            TIMEFRAME SELECTION RULES (STRICT)
            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

            Supported timeframes ONLY:
            â€¢ M5
            â€¢ M15
            â€¢ H1
            â€¢ H4

            âŒ Forbidden timeframes:
            â€¢ D1
            â€¢ W1
            â€¢ Any timeframe not explicitly listed above

            If the user explicitly specifies a timeframe:
            â†’ YOU MUST USE IT EXACTLY (after normalization)

            If the user does NOT specify a timeframe:
            â†’ Default to **H4**
            â†’ NEVER default to D1 or W1

            Normalization mapping (MANDATORY):
            â€¢ M5   â†’ "5min"
            â€¢ M15  â†’ "15min"
            â€¢ H1   â†’ "1h"
            â€¢ H4   â†’ "4h"

            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            STEP 1 â€” Market Data Collection (MANDATORY)
            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

            You MUST call: twelve_data_time_series

            Parameters:
            - symbol â†’ normalized canonical symbol (e.g. XAU/USD, EUR/USD, BTC/USD)
            - interval â†’ derived STRICTLY from the rules above
            - outputsize â†’ 100 unless user explicitly requests otherwise

            If the symbol is unclear:
            â†’ choose the closest canonical instrument
            â†’ DO NOT change the timeframe

            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
            FAILURE PREVENTION RULE
            â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

            NEVER widen the timeframe to â€œbe safeâ€.
            NEVER choose a higher timeframe than the user requested.
            NEVER infer D1 or W1 unless EXPLICITLY requested by the user.

        """),
        HumanMessage(content=f"""
            {(state.get("question") or "").strip()}

            "Generate a trade setup for " + { state.get("instrument") or "not specified" } + ".\n" +
            "Use the following optional parameters if provided:\n" +
            "- timeframe: " + { state.get("timeframe") or "not specified" } + "\n" +
            "- risk level: " + { state.get("riskLevel") or "not specified"} + "\n" +
            "- strategy: " + { state.get("strategy") or "not specified"} + "\n" +
            "- position size: " + { state.get("positionSize") or "not specified" } + "\n" +
            "Also take into account this custom note from the user if available: " + { state.get("customNotes") or "none" } + "\n\n" +
            "Please ensure that the macroeconomic context is well-developed and supported by citations from high-authority institutional sources when possible. Disregard low-authority content unless it supports a validated macro view."


            The "content.content" field contains the strategist report (baseline + enriched).
            The "fundamentals" field contains structured macro data (CPI, NFP, rates, positioning, etc.).
            The "citations_news" field contains original publisher sources.

            Use this information to produce structured trade setups as per your system prompt.
            """),
    ]

    return {"messages": [llm.invoke(messages)]}

def planner_forecast(state):
    llm = ChatOpenAI(
        model="gpt-4.1-mini",
        temperature=0,
    ).bind_tools([forecast_aws_api])

    messages = state["messages"] + [
        SystemMessage(content="""
                      
      You are a Trade Planner operating inside a LangGraph execution pipeline.

        Your role is STRICTLY LIMITED to orchestrating tool calls.
        You do NOT think, explain, summarize, or generate any narrative.
        You ONLY convert the USER REQUEST into a SEQUENCE OF TOOL CALLS.

        You are a deterministic orchestration node.

        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        ABSOLUTE EXECUTION CONTRACT
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

        For EVERY user request, you MUST execute EXACTLY this tool calls,
        IN THIS EXACT ORDER, WITHOUT EXCEPTION:

        1ï¸âƒ£ forecast_aws_api  

        This rule overrides ALL other considerations.

        Even if information is missing, ambiguous, or uncertain:
        â†’ You MUST still call this tools  
        â†’ You MUST infer conservative, valid defaults  

        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        SOURCE OF TRUTH (CRITICAL)
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

        The ONLY source of intent, parameters, and context is:
        â†’ the HumanMessage (user request)

        You MUST extract from it:
        - instrument
        - timeframe
        - trading intent (intraday / swing / position if inferable)
        - directional bias (if implied)
        - any constraint explicitly stated

        If something is NOT explicitly stated:
        â†’ infer the MOST CONSERVATIVE valid value
        â†’ NEVER skip a tool call   
                                
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        STEP 2 â€” Forecast & Trade Signal Collection (MANDATORY)
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

        You MUST call: forecast_aws_api

        Rules:
        - symbol â†’ SAME as STEP 1
        - timeframe â†’ SAME as STEP 1
        - horizons â†’ POSITIVE INTEGERS = NUMBER OF FUTURE CANDLES

        HORIZON MAPPING (STRICT):
        - 15m â†’ [12]
        - 30m â†’ [24]
        - 1h  â†’ [35]
        - 4h  â†’ [60]
                      
        TIMEFRAME CONSTRAINTS (STRICT â€” NON-NEGOTIABLE)

        The Forecast API ONLY supports the following execution timeframes:
        â€¢ "15min"
        â€¢ "30min"
        â€¢ "1h"
        â€¢ "4h"

        You MUST comply with these rules EXACTLY:

        1. Allowed values
        - timeframe âˆˆ {"15min", "30min", "1h", "4h"}
        - Any other value is FORBIDDEN.

        2. Normalization (MANDATORY)
        If the user provides:
        - "M15", "15m", "15 minutes" â†’ use "15min"
        - "M30", "30m", "30 minutes" â†’ use "30min"
        - "H1", "1H", "1 hour"       â†’ use "1h"
        - "H4", "4H", "4 hours"      â†’ use "4h"

        3. Invalid or unsupported timeframe
        If the user requests ANY other timeframe (e.g. "5m", "1d", "daily", "weekly"):
        - DO NOT invent or approximate
        - DO NOT ask a question
        - DO NOT stop execution
        - Select the CLOSEST CONSERVATIVE supported timeframe:
            â€¢ Intraday intent â†’ "15min"
            â€¢ Swing intent â†’ "1h"
            â€¢ Position / unclear â†’ "4h"

        4. Forecast API rule (CRITICAL)
        - The `forecast_aws_api` tool MUST always be called with one of the four supported values above.
        - Sending any other timeframe INVALIDATES the request.

        5. Consistency requirement
        - The SAME normalized timeframe MUST be used for:
            â€¢ twelve_data_time_series
            â€¢ forecast_aws_api
            â€¢ surface_probability_aws_api

        This rule OVERRIDES any user preference, example, or prior assumption.
        Failure to comply invalidates the execution.
            
        Intent refinement:
        - Intraday â†’ shortest valid horizon
        - Swing â†’ mid-range
        - Position â†’ longest valid

        If intent is unclear:
        â†’ choose the SHORTEST valid horizon

        NEVER use strings ("12h", "3d", etc.)
                      
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        1. CANONICAL INSTRUMENT FORMAT (MANDATORY)
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        All instruments MUST be normalized to the canonical format:

        â€¢ FX        â†’ "XXX/USD" or "USD/XXX"
        â€¢ Commodities â†’ predefined symbols only (e.g. "XAU/USD", "WTI/USD")
        â€¢ Crypto    â†’ "BTC/USD", "ETH/USD", etc.

        âŒ DO NOT use:
        - inverted aliases ("USDCAD", "CADUSD")
        - lowercase
        - free-text names ("Canadian Dollar", "CAD Dollar")
        - synthetic or unsupported pairs
                      
        INSTRUMENT NORMALIZATION & MODEL AVAILABILITY (STRICT)

        The Forecast & Surface APIs ONLY support instruments for which trained models
        are available in the backend.

        You MUST strictly normalize, validate, and filter instruments BEFORE
        calling any forecasting or surface-generation tool.

        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        1. CANONICAL INSTRUMENT FORMAT (MANDATORY)
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        All instruments MUST be converted to a canonical trading symbol:

        â€¢ FX        â†’ "XXX/USD" or "USD/XXX"
        â€¢ Commodities â†’ predefined canonical symbols only (e.g. "XAU/USD", "WTI/USD")
        â€¢ Crypto    â†’ "SYMBOL/USD" (e.g. BTC/USD, ETH/USD)

        âŒ Forbidden formats:
        - concatenated tickers (e.g. EURUSD, BTCUSDT)
        - lowercase symbols
        - descriptive names ("Euro", "Gold", "Bitcoin")
        - aliases, broker-specific codes, or synthetic names

        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        2. USER INPUT NORMALIZATION RULES
        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        If the user provides:
        - an alias â†’ normalize to canonical symbol
        - a single currency â†’ infer the most liquid canonical pair
        - a descriptive asset name â†’ map to its canonical trading symbol

        Normalization MUST always result in an explicit canonical symbol.


                      """),        
        HumanMessage(content=f"""
            {(state.get("question") or "").strip()}

            "Generate a trade setup for " + { state.get("instrument") or "not specified" } + ".\n" +
            "Use the following optional parameters if provided:\n" +
            "- timeframe: " + { state.get("timeframe") or "not specified" } + "\n" +
            "- risk level: " + { state.get("riskLevel") or "not specified"} + "\n" +
            "- strategy: " + { state.get("strategy") or "not specified"} + "\n" +
            "- position size: " + { state.get("positionSize") or "not specified" } + "\n" +
            "Also take into account this custom note from the user if available: " + { state.get("customNotes") or "none" } + "\n\n" +
            "Please ensure that the macroeconomic context is well-developed and supported by citations from high-authority institutional sources when possible. Disregard low-authority content unless it supports a validated macro view."


            The "content.content" field contains the strategist report (baseline + enriched).
            The "fundamentals" field contains structured macro data (CPI, NFP, rates, positioning, etc.).
            The "citations_news" field contains original publisher sources.

            Use this information to produce structured trade setups as per your system prompt.
            """),
    ]

    return {"messages": [llm.invoke(messages)]}

def planner_surface(state):
    llm = ChatOpenAI(
        model="gpt-4.1-mini",
        temperature=0,
    ).bind_tools([surface_probability_aws_api])

    messages = state["messages"] + [
        SystemMessage(content="""
        You are a Trade Planner operating inside a LangGraph execution pipeline.

        Your role is STRICTLY LIMITED to orchestrating tool calls.
        You do NOT think, explain, summarize, or generate any narrative.
        You ONLY convert the USER REQUEST into a SEQUENCE OF TOOL CALLS.

        You are a deterministic orchestration node.

        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        ABSOLUTE EXECUTION CONTRACT
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

        For EVERY user request, you MUST execute EXACTLY this tool calls,
        IN THIS EXACT ORDER, WITHOUT EXCEPTION:

        1ï¸âƒ£ surface_probability_aws_api  

        This rule overrides ALL other considerations.

        Even if information is missing, ambiguous, or uncertain:
        â†’ You MUST still call this tools  
        â†’ You MUST infer conservative, valid defaults  

        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        SOURCE OF TRUTH (CRITICAL)
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

        The ONLY source of intent, parameters, and context is:
        â†’ the HumanMessage (user request)

        You MUST extract from it:
        - instrument
        - timeframe
        - trading intent (intraday / swing / position if inferable)
        - directional bias (if implied)
        - any constraint explicitly stated

        If something is NOT explicitly stated:
        â†’ infer the MOST CONSERVATIVE valid value
        â†’ NEVER skip a tool call   
                                
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
        STEP â€” Target Probability Surface Generation (MANDATORY)
        â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”

        You MUST call: surface_probability_aws_api

        Rules:
        - symbol â†’ SAME as STEP 1
        - timeframe â†’ SAME as STEP 1
        - methodology â†’ "research" (ALWAYS)
        - direction â†’ inferred from forecast bias
        (if unclear â†’ default to "long")
        - paths â†’ 3000
        - horizon_hours OR steps â†’
        derived consistently from STEP 2 horizons
        (NEVER provide both)

        Range defaults (ALWAYS VALID):
        - target_prob:
            min = 0.4
            max = 0.8
            steps = 7
        - sl_sigma:
            min = 0.5
            max = 3.0
            steps = 7

        If entry_price is not provided:
        â†’ OMIT it and let the API resolve it


        """),        
        HumanMessage(content=f"""
            {(state.get("question") or "").strip()}

            "Generate a trade setup for " + { state.get("instrument") or "not specified" } + ".\n" +
            "Use the following optional parameters if provided:\n" +
            "- timeframe: " + { state.get("timeframe") or "not specified" } + "\n" +
            "- risk level: " + { state.get("riskLevel") or "not specified"} + "\n" +
            "- strategy: " + { state.get("strategy") or "not specified"} + "\n" +
            "- position size: " + { state.get("positionSize") or "not specified" } + "\n" +
            "Also take into account this custom note from the user if available: " + { state.get("customNotes") or "none" } + "\n\n" +
            "Please ensure that the macroeconomic context is well-developed and supported by citations from high-authority institutional sources when possible. Disregard low-authority content unless it supports a validated macro view."


            The "content.content" field contains the strategist report (baseline + enriched).
            The "fundamentals" field contains structured macro data (CPI, NFP, rates, positioning, etc.).
            The "citations_news" field contains original publisher sources.

            Use this information to produce structured trade setups as per your system prompt.
            """),
    ]

    return {"messages": [llm.invoke(messages)]}

@tool
async def twelve_data_time_series(
    symbol: str,
    interval: str,
    outputsize: int = 100,
) -> Dict[str, Any]:
    """
    Tool to fetch time series data from Twelve Data API.
    """
    start = time.time()
    URL = "https://api.twelvedata.com/time_series"

    params = {
        "apikey": os.environ["TWELVE_DATA_API_KEY"],
        "symbol": symbol,
        "interval": interval,
        "outputsize": outputsize,
        "format": "JSON",
    }

    print("[MACRO_COMMENTARY]twelve_data_time_series")

    async with httpx.AsyncClient() as client:
        r = await client.get(URL, params=params, timeout=30)
        r.raise_for_status()
        print(f"Twelve Data fetched {symbol} {interval} data.")
        # print(r.json())
        end = time.time()
        print(f"Twelve Data API call completed in {end - start:.2f} seconds.")
        return r.json()

def generate_trade_agent(state: DirectionState) -> DirectionState:
    start = time.time()

    llm = ChatOpenAI(
        model="gpt-4.1-mini",
        temperature=0,
    )

    resp = None
    error_reason = None

    system_prompt = f"""
Today's date is the {today_iso} and You are an institutional-grade trade setup generator for FX, crypto, and commodities. 
Your reasoning MUST rest on the following Market Commentary layer enriched 
with fundamentals and citations_news + forecast and trade signals from our forecasting system:
FORECAST DATA FROM TRADE GENERATION ENGINE:
{json.dumps(state.get("forecast_data"), indent=2)}

MARKET DATA FROM TWELVE DATA :
{json.dumps(state.get("market_data"), indent=2)}

and our Partner ABCG Research insights:
{json.dumps(state.get("abcg_research"), ensure_ascii=False)}

You must:
- Always provide: entryPrice, stopLoss, takeProfits[], riskRewardRatio.
- Tailor to user inputs when provided (instrument, timeframe, riskLevel, strategy, positionSize, customNotes). 
  If not provided, propose a DEFAULT PANEL across horizons: Scalping (5â€“15m), Intraday (H1/H4), Swing (D1/W1), Position (multi-week).
- Contextualize each trade with institutional logic: why now, which macro drivers, how technicals align, event risks, suitable horizon, expected volatility regime.
- If directional bias is given in Market Commentary, align setups to it unless fresh fundamentals argue otherwise.
- Respect sourcing: commentary narrative is anchor; fundamentals JSON provides validation; citations_news provide context.
- Tone: institutional, structured, confident. Add a disclaimer at the end: "Illustrative ideas, not investment advice."

You MUST strictly follow this JSON schema:
{format_instructions}

Additional rules:
- Percentages must be numeric or strings, never symbols (no % outside quotes)
- No trailing commas
- No markdown
- No text outside JSON

Validation rules:
- Use supports/resistances and bias from commentary if available.
- Use fundamentals JSON values when present (e.g., CPI, NFP, rates, RSI).
- Do NOT invent numbers if missing. Leave empty arrays.
- If commentary bias conflicts with fundamentals, flag in 'context' and propose conservative setup.

CRITICAL OUTPUT RULES:
- Return RAW JSON only, no markdown, no quotes, UTF-8 safe
- Do NOT wrap the JSON in markdown
- Do NOT use ```json or ```
- Do NOT add explanations or text before or after
- The first character MUST be '{{'
- The last character MUST be '}}'
"""

    user_prompt = f"""
{(state.get("question") or "").strip()}

          "Generate a trade setup for " + { state.get("instrument") or "not specified" } + ".\n" +
          "Use the following optional parameters if provided:\n" +
          "- timeframe: " + { state.get("timeframe") or "not specified" } + "\n" +
          "- risk level: " + { state.get("riskLevel") or "not specified"} + "\n" +
          "- strategy: " + { state.get("strategy") or "not specified"} + "\n" +
          "- position size: " + { state.get("positionSize") or "not specified" } + "\n" +
          "Also take into account this custom note from the user if available: " + { state.get("customNotes") or "none" } + "\n\n" +
          "Please ensure that the macroeconomic context is well-developed and supported by citations from high-authority institutional sources when possible. Disregard low-authority content unless it supports a validated macro view."


The "content.content" field contains the strategist report (baseline + enriched).
The "fundamentals" field contains structured macro data (CPI, NFP, rates, positioning, etc.).
The "citations_news" field contains original publisher sources.

Use this information to produce structured trade setups as per your system prompt.
"""

    try:

        resp = llm.invoke(state["messages"] + [
            SystemMessage(content=system_prompt),
            HumanMessage(content=user_prompt)
        ])

    except Exception as e:
        error_reason = f"LLM invocation failed: {str(e)}"

    end = time.time()
    print(f"Generated trade agent completed in {end - start:.2f} seconds.")

    # -----------------------------
    # ğŸ›‘ CASE 1 â€” Hard failure
    # -----------------------------
    if resp is None:
        print("âš ï¸ Final agent LLM invocation failed")
        st = supabase_update_job_status(state,{
            "trade_generation_output": {
                "final_answer": "Unavailable",
                "confidence_note": "Empty LLM response",
            }},            
            "error",)
        return {
            "trade_generation_output": {
                "final_answer": "Unavailable",
                "trade_setup": "Unavailable",
                "confidence_note": "LLM call failed",
                "error": error_reason,
            }
        }

    # -----------------------------
    # ğŸ›‘ CASE 2 â€” Tool call only
    # -----------------------------
    if getattr(resp, "tool_calls", None):
        print("âš ï¸ Final agent emitted tool calls instead of content")
        print("Tool calls:", resp.tool_calls)
        return {
            "trade_generation_output": {
                "final_answer": (
                    "Market data requested via tool call. "
                    "Final narrative could not be generated at this stage."
                ),
                "trade_setup": "Awaiting tool execution",
                "confidence_note": "Deferred â€” awaiting tool execution",
                "tool_calls": resp.tool_calls,
            }
        }

    # -----------------------------
    # ğŸ›‘ CASE 3 â€” Empty content
    # -----------------------------
    content = (resp.content or "").strip()
    if not content:
        print("âš ï¸ Empty LLM content")
        st = supabase_update_job_status(state, {
                    "final_answer": "Unavailable",
                    "confidence_note": "Empty LLM response",
                },
                "error")
        return {
            "trade_generation_output": {
                "final_answer": "Unavailable",
                "confidence_note": "Empty LLM response",
            }
        }

    # -----------------------------
    # âœ… CASE 4 â€” Normal path
    # -----------------------------

    print("âœ… Final agent output content received")
    # print("content:", content)
    st = supabase_update_job_status(state,{ "final_answer": content,
        "trade_setup": {json.dumps(state.get('forecast_data'))},
        "risk_surface": {json.dumps(state.get('surface_data'))}})
    return {
        "trade_generation_output": {
            "final_answer": content,
            "trade_setup": {json.dumps(state.get('forecast_data'))},
            "risk_surface": {json.dumps(state.get('surface_data'))},
            "confidence_note": (
                "Partner research unavailable"
                if isinstance(state.get("abcg_research"), dict)
                and state["abcg_research"].get("status") == "unavailable"
                else "Partner research integrated"
            )
        }
    }

import os
from cerebras.cloud.sdk import Cerebras

def final_synthesis_agent(state: DirectionState) -> DirectionState:
    start = time.time()
    print("[MACRO_COMMENTARY]final_synthesis_agent")

    market = None
    resp: Optional[Any] = None
    error_reason: Optional[str] = None
    res: Optional[str] = None 

    client = Cerebras(
        # This is the default and can be omitted
        api_key=os.environ.get("CEREBRAS_API_KEY")
    )
    # print("*****************************************************")
    # print("Final synthesis has been called")
    # print(state["messages"])
    # print("*****************************************************")
    parser = PydanticOutputParser(pydantic_object=StrategistOutput)

    system_prompt = f"""

    Todayâ€™s date is: { today_iso } and You are a senior FX/macro strategist at a top-tier macro research firm with WEB BROWSING ENABLED.  

    Your mandate is THREE-FOLD in a SINGLE OUTPUT:  

    1. **Market Data Collection**  
        TOOL RESPONSE MESSAGE HISTORY:
        ---
        {json.dumps(state.get("market_data"), ensure_ascii=False)}
        ---

    âš ï¸ Remember: **all this market data is contextual metadata. The only mandatory final output is the `content` field.**

    2. **Strategist Report Construction (Weekly Outlook)**  
    - Use Partner institutional insights (ABCG RESEARCH) as foundation. Expand clearly.  
    - Complement with contextual news discovered via Perplexity, but cite only the original publishers.  
    - **Tone must be institutional, structured, confident (Goldman Sachs / JPMorgan style).**  
    - **Content base** â†’ Anchor in Partner Research. Expand clearly.  
    - **News complement** â†’ Enrich with news (discovered via Perplexity) but always cite only the original publishers.  

    - Mandatory structure:  

    ---

    Executive Summary  
    {{2â€“3 sentences}}  

    Fundamental Analysis  
    {{Narrative + bullets}}  

    Directional Bias  
    {{Bullish/Bearish/Neutral}} 
    Confidence: "{{XX}}%" 

    Key Levels  
    Support  
    {{level1}}  
    {{level2}}  
    Resistance  
    {{level1}}  
    {{level2}}  
    
    AI Insights Breakdown  
    Toggle GPT  
    {{GPT narrative}} 

    Toggle Curated  
    {{Institutional view}}  
    ---  

    3. **Fundamentals Enrichment**  
    - All macroeconomic fundamentals (releases, actual, consensus, previous, timestamps) must come exclusively from the Finnhub Economic Calendar API.  
    - If an event is not present in Finnhub, mark it `"Unavailable"`.  
    - Perplexity or other news sources may only be used for qualitative context (commentary, sentiment), never for datapoints.
    - Always include: indicator, actual, consensus, previous, release timestamp, checked_at.  
    - If web access fails, add `"warning": "web access to FF/TE unavailable"`.  
    - **Enriched note must be the base_report enriched with:**  
        - inline numeric clarifications,  
        - appended sections: Fundamentals, Rate Differentials, Positioning, Balance of Payments, Central Bank Pricing, Sentiment Drivers, Event Watch.  
    - âš ï¸ **This enriched strategist note must always be placed in the `content` field.  
        The `content` field is the one and only mandatory narrative output.  
        All other fields (request, market_data, meta, base_report, fundamentals, citations_news) are supporting context and traceability only.**

    ---

    ## OUTPUT RULES

    - Produce a single JSON

    âš ï¸ Important: The list of collected_intervals and series timeslots above is only an example.  
    Always decide dynamically which intervals to collect depending on the userâ€™s query and trading horizon.  
    Do not fetch redundant data if it is not required.  

    âš ï¸ Critical: The field `content` must always contain the **final enriched strategist note** and be at the VERY ROOT OF THE JSON OUTPUT.  
    This is the **main output of your work**.  
    All other fields are optional context, metadata, and traceability, but `content` is the only mandatory narrative output.  

    Every field must exist in the JSON (if data is missing, use empty string, empty array, or `"Unavailable"`).  
    Return only JSON, no free text.  

    ---
    âš ï¸ Temporal Truth Rule:  
    At any point in the reasoning, the authoritative state of macroeconomic fundamentals is the Finnhub Economic Calendar snapshot provided at runtime ( see below )  
    This snapshot reflects the truth of the economic environment at todayâ€™s date { today_iso }.  
    All strategist reasoning must align strictly with this snapshot, treating it as the ground truth of the economy at time T.  
    If any discrepancy arises between other sources and Finnhub, Finnhub always prevails.

    ## INPUTS

    - Perplexity discovery: {state.get("articles")} 
    - Partner research: {json.dumps(state.get("abcg_research"), ensure_ascii=False)}
    - User query: { (state.get("question") or "").strip() }
    - Finnhub economic calendar : || {json.dumps(state.get("economic_calendar_agent"), ensure_ascii=False)} ||

    ---

    ## CRITICAL RULES

    - Market data â†’ ONLY Twelve Data (valid intervals only).  
    - Fundamentals â†’ ONLY ForexFactory/TradingEconomics.  
    - News â†’ ONLY original publishers.  
    - Ranges 3d/5d must always be computed locally from 1day data (never API calls).  
    - Missing sections or invalid JSON will invalidate your output.  

    Do not mention or reveal any third-party data vendors or platforms ( finnhub twelve, data, tradingview) when 
    attribution is needed, use generic phrasing while keeping the existing JSON structure unchanged 

    CRITICAL OUTPUT RULES:

    - Return RAW JSON only, no markdown, no quotes, UTF-8 safe
    - Do NOT wrap the JSON in markdown
    - Do NOT use ```json or ```
    - Do NOT add explanations or text before or after
    - The first character MUST be '{{'
    - The last character MUST be '}}'
    
    You MUST return valid JSON following EXACTLY this schema.
        that follows EXACTLY this schema:

        {parser.get_format_instructions()}
    Rules:
        - All percentages MUST be strings (e.g. "85%")
        - Never use trailing commas
        - Never include comments
        - Never include text outside JSON
    """

    # print("**************STATE**************")
    # print(system_prompt)
    # print("*********************************")


    user_prompt = f"""
    {(state.get("question") or "").strip()}
    """

    try:
        stream = client.chat.completions.create(
            messages=[
                {
                    "role": "system",
                    "content": system_prompt
                },
                            {
                    "role": "user",
                    "content": user_prompt
                }
            ],
            model="llama-3.3-70b",
            stream=True,
            max_completion_tokens=65000,
            temperature=1,
            top_p=0.95
        )

        res = ""
        for chunk in stream:
            # print(chunk.choices[0].delta.content or "", end="")
            res += chunk.choices[0].delta.content or ""

    except Exception as e:
        error_reason = f"LLM invocation failed: {str(e)}"

    end = time.time()
    # print("remember user s question was :", (state["body"].get("question") or "").strip())
    print(f"Final Synthesis Agent completed in {end - start:.2f} seconds.")

    # -----------------------------
    # ğŸ›‘ CASE 1 â€” Hard failure
    # -----------------------------
    if res is None:
        print("âš ï¸ Final agent LLM invocation failed")
        st = supabase_update_job_status(state, {
                "final_answer": "Unavailable",
                "confidence_note": "LLM call failed",
                "error": error_reason,
        },
        "error",
        )
        return {
            "output": {
                "final_answer": "Unavailable",
                "confidence_note": "LLM call failed",
                "error": error_reason,
            }
        }

    # -----------------------------
    # ğŸ›‘ CASE 2 â€” Tool call only
    # -----------------------------

    # -----------------------------
    # ğŸ›‘ CASE 3 â€” Empty content
    # -----------------------------
    content = (res or "").strip()
    if not content:
        print("âš ï¸ Empty LLM content")
        t = supabase_update_job_status(state, {
                "final_answer": "Unavailable",
                "confidence_note": "Empty LLM response",
            },
            "error")
        return {
            "output": {
                "final_answer": "Unavailable",
                "confidence_note": "Empty LLM response",
            }
        }

    # -----------------------------
    # âœ… CASE 4 â€” Normal path
    # -----------------------------
    st = supabase_update_job_status(state,content.strip())
    return {
        "output": {
            "final_answer": content.strip(),
            "confidence_note": (
                "Partner research unavailable"
                if isinstance(state.get("abcg_research"), dict)
                and state["abcg_research"].get("status") == "unavailable"
                else "Partner research integrated"
            )
        }
    }

async def economic_calendar_agent(state: DirectionState) -> Dict[str, Any]:
    start = time.time()
    import httpx
    from datetime import datetime, timedelta, timezone
    import re

    QUESTION_RAW = (state["question"] or "").strip()
    QUESTION = QUESTION_RAW.upper()
    NOW = datetime.now(timezone.utc)

    # ------------------------------
    # Helpers: dates
    # ------------------------------
    def to_ymd(d: datetime) -> str:
        return d.strftime("%Y-%m-%d")

    def compute_range():
        from_d = NOW
        to_d = NOW

        if "NEXT WEEK" in QUESTION:
            from_d -= timedelta(days=7)
            to_d += timedelta(days=14)
        elif "LAST MONTH" in QUESTION:
            from_d -= timedelta(days=30)
        else:
            from_d -= timedelta(days=7)
            to_d += timedelta(days=7)

        return to_ymd(from_d), to_ymd(to_d)

    # ------------------------------
    # Detect countries
    # ------------------------------
    CCY_TO_COUNTRY = {
        "USD": "US", "EUR": "EU", "GBP": "UK", "JPY": "JP",
        "CHF": "CH", "AUD": "AU", "CAD": "CA", "NZD": "NZ", "CNY": "CN",
    }

    countries = set()

    for a, b in re.findall(r"([A-Z]{3})/([A-Z]{3})", QUESTION):
        if a in CCY_TO_COUNTRY: countries.add(CCY_TO_COUNTRY[a])
        if b in CCY_TO_COUNTRY: countries.add(CCY_TO_COUNTRY[b])

    for ccy, ctry in CCY_TO_COUNTRY.items():
        if ccy in QUESTION:
            countries.add(ctry)

    if not countries:
        countries = {"US", "EU"}

    # ------------------------------
    # Event filters
    # ------------------------------
    EVENT_MAP = {
        "CPI": ["CPI", "INFLATION"],
        "NFP": ["NONFARM", "PAYROLL", "NFP"],
        "GDP": ["GDP"],
        "PMI": ["PMI"],
        "RATES": ["RATE", "FOMC", "ECB", "BOE", "BOJ"],
        "JOBS": ["UNEMPLOY", "EMPLOY"],
    }

    requested = [
        k for k, pats in EVENT_MAP.items()
        if any(p in QUESTION for p in pats)
    ]

    def match_event(name: str) -> bool:
        if not requested:
            return True
        name = name.upper()
        return any(
            any(p in name for p in EVENT_MAP[k])
            for k in requested
        )

    def impact_allowed(impact: str) -> bool:
        if impact in ("high", "medium"):
            return True
        return impact == "low" and "LOW" in QUESTION

    def to_iso(ts: str):
        try:
            return datetime.fromisoformat(ts.replace(" ", "T") + "Z").isoformat()
        except Exception:
            return "Unavailable"

    def val(v):
        return "N/A" if v in (None, "", []) else str(v)

    from_d, to_d = compute_range()
    FINNHUB_URL = "https://finnhub.io/api/v1/calendar/economic"
    FINNHUB_TOKEN = os.environ["FINNHUB_TOKEN"]
    TOKEN = FINNHUB_TOKEN
    # TOKEN = state["body"].get(
    #     "finnhubToken",
    #     ""
    # )

    try:
        start = time.time()
        async with httpx.AsyncClient(timeout=30) as client:
            r = await client.get(
                FINNHUB_URL,
                params={"token": TOKEN, "from": from_d, "to": to_d},
            )
            r.raise_for_status()
            calendar = r.json().get("economicCalendar", [])

        filtered = [
            {
                "event": ev.get("event", "Unavailable"),
                "country": ev.get("country", "Unavailable"),
                "time": to_iso(ev.get("time")),
                "actual": val(ev.get("actual")),
                "consensus": val(ev.get("estimate")),
                "previous": val(ev.get("prev")),
                "impact": (ev.get("impact") or "Unavailable").lower(),
            }
            for ev in calendar
            if ev.get("country") in countries
            and impact_allowed((ev.get("impact") or "").lower())
            and match_event(ev.get("event", ""))
        ][:10]
        end = time.time()
        print(f"Economic Calendar Agent completed in {end - start:.2f} seconds.")
        return {
            "economic_calendar": {
                "economic_events": filtered or "Unavailable",
                "meta": {
                    "detected_countries": list(countries),
                    "requested_event_filters": requested or "ALL",
                    "date_range": {"from": from_d, "to": to_d},
                    "total_events_received": len(calendar),
                    "total_events_kept": len(filtered),
                },
            }
        }

    except Exception as e:
        end= time.time()
        print(f"Economic Calendar Agent failed in {end - start:.2f} seconds.")
        return {
            "economic_calendar": {
                "economic_events": "Unavailable",
                "meta": {
                    "error": {
                        "message": str(e),
                        "checked_at": datetime.utcnow().isoformat(),
                        "source": FINNHUB_URL,
                    }
                },
            }
        }

from langchain_core.messages import AIMessage, ToolMessage
import json

def extract_tool_outputs(state: DirectionState) -> dict:
    tool_call_id_to_name = {}
    forecast = None
    market = None
    surface = None

    print("extract has been called")

    # 1ï¸âƒ£ rÃ©cupÃ©rer les tool_calls depuis les AIMessage
    for m in state["messages"]:
        if isinstance(m, AIMessage) and m.tool_calls:
            for tc in m.tool_calls:
                tool_call_id_to_name[tc["id"]] = tc["name"]

    # 2ï¸âƒ£ lire les ToolMessage et matcher via tool_call_id
    for m in state["messages"]:
        if isinstance(m, ToolMessage):
            tool_name = tool_call_id_to_name.get(m.tool_call_id)

            if tool_name == "forecast_aws_api":
                forecast = json.loads(m.content)
                print("***********************")
                print(f"forecast_aws_api TOUCHE")
                # print(forecast)
                print("***********************")

            elif tool_name == "twelve_data_time_series":
                market = json.loads(m.content)
                print("***********************")
                print(f"twelve_data_time_series TOUCHE")
                print("***********************")

            elif tool_name == "surface_probability_aws_api":
                surface = json.loads(m.content)
                print("***********************")
                print(f"surface_probability_aws_api TOUCHE")
                print("***********************") 

    # print("************************STATE MESSAGE****************************")
    # print(state["messages"])
    # print("************************STATE MESSAGE****************************")

    # print("************************FORECAST MESSAGE****************************")
    # print(forecast)
    # print("************************FORECAST MESSAGE****************************")

    # print("************************MARKET MESSAGE****************************")
    # print(market)
    # print("************************MARKET MESSAGE****************************")

    # print("************************SURFACE MESSAGE****************************")
    # print(surface)
    # print("************************SURFACE MESSAGE****************************")
    return {
        "forecast_data": forecast,
        "market_data": market,
        "surface_data": surface,
    }

def start_node(state: DirectionState) -> DirectionState:
    return {}

def start_trade_queued_node(state: DirectionState) -> DirectionState:
    return {}

def extract_first_json(text: str) -> dict:
    """
    Extract and parse the first valid JSON object from a string.
    Safe against:
    - extra text before/after
    - multiple concatenated JSON objects
    """
    decoder = json.JSONDecoder()
    text = text.strip()

    try:
        obj, idx = decoder.raw_decode(text)
        return obj
    except json.JSONDecodeError as e:
        raise ValueError(f"No valid JSON found: {e}") from e

def build_run_graph():
    graph = StateGraph(DirectionState)
    graph.add_node("start", start_node)
    graph.add_node("fanout", lambda s: s)
    graph.add_node("data_collection_llm", data_collection_llm_agent)
    graph.add_node("abcg_research", abcg_research_agent)
    graph.add_node("economic_calendar", economic_calendar_agent)
    graph.add_node("planner", planner_agent)
    graph.add_node("tools", ToolNode([twelve_data_time_series]))
    graph.add_node("extract_tool_outputs", extract_tool_outputs)
    graph.add_node("final", final_synthesis_agent)

    # router
    graph.set_entry_point("start")
 
    graph.add_edge("start", "fanout")
    graph.add_edge("fanout", "data_collection_llm")
    graph.add_edge("fanout", "abcg_research")
    graph.add_edge("fanout", "economic_calendar")

    graph.add_edge("data_collection_llm", "planner")
    graph.add_edge("abcg_research", "planner")
    graph.add_edge("economic_calendar", "planner")
    graph.add_edge("planner", "tools")
    graph.add_edge("tools","extract_tool_outputs")
    graph.add_edge("extract_tool_outputs", "final")
    graph.add_edge("final", END)

    return graph.compile()

def build_trade_graph():
    graph = StateGraph(DirectionState)
    graph.add_node("start", start_trade_queued_node)
    graph.add_node("fanout", lambda s: s)
    graph.add_node("data_collection_llm", data_collection_llm_agent)
    graph.add_node("abcg_research", abcg_research_agent)
    graph.add_node("economic_calendar", economic_calendar_agent)
    graph.add_node("planner", planner_trade_agent)
    graph.add_node("tools", ToolNode([forecast_aws_api,twelve_data_time_series,surface_probability_aws_api]))
    graph.add_node("extract_tool_outputs", extract_tool_outputs)
    graph.add_node("planner_trade_agent", planner_agent)
    graph.add_node("final", generate_trade_agent)    

    # router
    graph.set_entry_point("start")

    graph.add_edge("start", "fanout")
    graph.add_edge("fanout", "data_collection_llm")
    graph.add_edge("fanout", "abcg_research")
    graph.add_edge("fanout", "economic_calendar")

    graph.add_edge("data_collection_llm", "planner")
    graph.add_edge("abcg_research", "planner")
    graph.add_edge("economic_calendar", "planner")
    graph.add_edge("planner", "tools")
    graph.add_edge("tools", "extract_tool_outputs")
    graph.add_edge("extract_tool_outputs", "final")
    graph.add_edge("final", END)
    print("Trade graph built.")
    return graph.compile()

def build_trade2_graph():
    graph = StateGraph(DirectionState)
    graph.add_node("start", start_trade_queued_node)
    graph.add_node("fanout", lambda s: s)
    graph.add_node("fanout2", lambda s: s)

    graph.add_node("data_collection_llm", data_collection_llm_agent)
    graph.add_node("abcg_research", abcg_research_agent)
    graph.add_node("economic_calendar", economic_calendar_agent)
    graph.add_node("planner", planner_trade_agent)
    graph.add_node("planner_market", planner_market_data)
    graph.add_node("planner_forecast", planner_forecast)
    graph.add_node("planner_surface", planner_surface)
    graph.add_node("tools_market", ToolNode([twelve_data_time_series]))
    graph.add_node("tools_forecast", ToolNode([forecast_aws_api]))
    graph.add_node("tools_surface", ToolNode([surface_probability_aws_api]))
    graph.add_node("extract_tool_outputs", extract_tool_outputs)
    graph.add_node("planner_trade_agent", planner_agent)
    graph.add_node("final", generate_trade_agent)    

    # router
    graph.set_entry_point("start")

    graph.add_edge("start", "fanout")
    graph.add_edge("fanout", "data_collection_llm")
    graph.add_edge("fanout", "abcg_research")
    graph.add_edge("fanout", "economic_calendar")

    graph.add_edge("data_collection_llm", "fanout2")
    graph.add_edge("abcg_research", "fanout2")
    graph.add_edge("economic_calendar", "fanout2")
    graph.add_edge("fanout2", "planner_market")
    
    graph.add_edge("planner_market", "tools_market")
    graph.add_edge("tools_market", "planner_forecast")
    graph.add_edge("planner_forecast", "tools_forecast")
    graph.add_edge("tools_forecast", "planner_surface")
    graph.add_edge("planner_surface", "tools_surface")
    graph.add_edge("tools_surface", "extract_tool_outputs")

    graph.add_edge("extract_tool_outputs", "final")
    graph.add_edge("final", END)
    print("Trade v2. graph built.")
    return graph.compile()

def make_supabase_update_node(table: str, fields: dict, filter_key: str):
    start = time.time()
    supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
    def node(state: dict) -> dict:
        body = state.get("body", {})
        value = body.get(filter_key)

        if not value:
            raise ValueError(f"{filter_key} missing")

        supabase.table(table).update(fields).eq("id", value).execute()
        return state
    end = time.time()
    print(f"Supabase update node created in {end - start:.2f} seconds.")
    return node

def extract_first_json(text: str) -> dict:
    """
    Extract and parse the first valid JSON object from a string.
    Safe against:
    - extra text before/after
    - multiple concatenated JSON objects
    """
    decoder = json.JSONDecoder()
    text = text.strip()

    try:
        obj, idx = decoder.raw_decode(text)
        return obj
    except json.JSONDecodeError as e:
        raise ValueError(f"No valid JSON found: {e}") from e
    
def has_nested_key(d: dict, keys: list[str]) -> bool:
    current = d
    for k in keys:
        if not isinstance(current, dict):
            return False
        if k not in current:
            return False
        current = current[k]
    return True


@app.post("/run")
async def run_webhook(request: Request):
    """
    AWS Lambda entrypoint
    """
    try:

        start = time.time()
        body = await request.json()
        job_id = body.get("job_id")
        if isinstance(body, str):
            body = json.loads(body)

        state = {
            **body,
            "body": body, 
            "job_id": job_id,
            "messages": [],
            "tradeMessages": [],
            "articles": [],
            "macro_insight": "",
            "abcg_research": None,
            "trade_setup": None,
            "economic_calendar": None,
            "market_data": None,
            "output": {},
            "error": None
        }

        # state = body

        # print("********************BODY********************")
        # print(body)
        # print("********************BODY********************")

        if body.get("mode", "") == "trade_generation":
            result = await build_trade2_graph().ainvoke(state)
            raw = result["trade_generation_output"]
            # print("********************RISK_SURFACE********************")
            # print(has_nested_key(raw,["risk_surface"]))
            # print("********************RISK_SURFACE********************")
        else:
            result = await build_run_graph().ainvoke(state)
            raw = result["output"]["final_answer"]

        # print("[MAIN]Parsed LLM output:", raw)

        if isinstance(raw, str):
            clean = strip_json_fences(raw)
            parsed = json.loads(clean)
        elif isinstance(raw, dict):
            parsed = raw
        else:
            raise TypeError(f"Unexpected output type: {type(raw)}")
        print(f"[macro]type of raw is {type(parsed)}")

        supabase_response = supabase_update_job_status(state,{ 
                "job_id" : job_id,
                "message": {"content" : { "content": raw }} }
                )
        
        print(f"Supabase response status : {supabase_response}")
        
        # clean = strif_json_fences(raw)
        # parsed = json.loads(clean)

        end = time.time()
        print(f"Total Lambda execution completed in {end - start:.2f} seconds.")
        return {
            "statusCode": 200,
            "headers": {"Content-Type": "application/json"},
            "body": { 
                "message" : { 
                "status" : "done",
                "job_id" : job_id,
                "message": {"content" : { "content": raw}} }
                },
            "message": { 
                "message" : { 
                "status" : "done",
                "job_id" : job_id,
                "message": {"content" : { "content": raw  }} }
                }
        }

    except Exception as e:
        supabase_response = supabase_update_job_status(state,{ 
            "job_id" : job_id,
            "message": {"content" : { "content": raw }} },
            "error"
            )
        return {
            "statusCode": 500,
            "body": json.dumps({
                "error": str(e)
            })
        }